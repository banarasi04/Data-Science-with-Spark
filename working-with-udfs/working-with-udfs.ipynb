{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with UDFs with Apache Spark (PySpark)\n",
    "\n",
    "In this notebook we will extensively go through working with UDFs with Spark SQL as well as Spark DF API. We will learn how to write custom UDFs and register, register UDFs using lambda function without explicitly writing functions, how to write custom UDFs that take more than one parameter apart from the column names and also hack into PySpark source codes to write our own Custom UDFs.\n",
    "\n",
    "`\n",
    "@author: Anindya Saha  \n",
    "@email: mail.anindya@gmail.com\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "A brief synopsis of what each UDF use case is and what functionality does it touch on.\n",
    "\n",
    "| Section                                                                             |        Demonstrates |\n",
    "|:------------------------------------------------------------------------------------|:--------------------|\n",
    "|[1. Understanding the Data Set](#1.-Understanding-the-Data-Set:)||\n",
    "|[2. Creating the Spark Session](#2.-Creating-the-Spark-Session:)||\n",
    "|[3. Load the Data From Files Into DataFrames](#3.-Load-the-Data-From-Files-Into-DataFrames:)||\n",
    "|[3.1. Fix the Headers and Provide a Schema](#3.1.-Fix-the-Headers-and-Provide-a-Schema:)|Providing Custom Schema to Text File|\n",
    "|[4.1. Extract Columns using In-built Regex Functions](#4.1.-Extract-Columns-using-In-built-Regex-Functions:)|Using REGEXP_EXTRACT from pyspark.sql.functions on DF API|\n",
    "|[4.2 Extract Columns using Custom Regex Functions](#4.2-Extract-Columns-using-Custom-Regex-Functions:)||\n",
    "|[4.2.1. Specific Extraction Functions](#4.2.1.-Specific-Extraction-Functions:)|Use Regex to Extract Specific Field|\n",
    "|[4.2.2. Generic Extraction Functions](#4.2.2.-Generic-Extraction-Functions:)|Custom Regex functions taking Extra Parameter|\n",
    "|[4.2.3. Register the function as an UDF to be used with DF API](#4.2.3.-Register-the-function-as-an-UDF-to-be-used-with-DF-API:)|Using UDF with DF API|\n",
    "|[4.2.4. Curry up function that takes more than One Parameter](#4.2.4.-Curry-up-function-that-takes-more-than-One-Parameter:)|Workaround for DF API with Custom UDF that take extra Parameter beside Column name|\n",
    "|[4.2.5. Register UDF on the Fly](#4.2.5.-Register-UDF-on-the-Fly:)|Create Anonymous UDFs for DF API|\n",
    "|[4.2.6. Hack PySpark Codes to call Scala functions directly](#4.2.6.-Hack-PySpark-Codes-to-call-Scala-functions-directly:)|Drawing inspiration from PySpark source codes|\n",
    "|[4.3. Putting together everything in one place](#4.3.-Putting-together-everything-in-one-place:)|Concluding the DF API use case|\n",
    "|[5.1. Extract Columns using In-built Regex Functions](#5.1.-Extract-Columns-using-In-built-Regex-Functions:)|Using REGEXP_EXTRACT from pyspark.sql.functions on Spark SQL|\n",
    "|[5.2. Extract Columns using Custom Regex Functions](#5.2.-Extract-Columns-using-Custom-Regex-Functions:)||\n",
    "|[5.2.1. Register the function to the SQLContext as an udf to be used within SQL:](#5.2.1.-Register-the-function-to-the-SQLContext-as-an-udf-to-be-used-within-SQL:)|Using UDF with Spark SQL|\n",
    "|[5.2.2. Function that takes more than One Parameter](#5.2.2.-Function-that-takes-more-than-One-Parameter:)|Spark SQL support Custom UDF that take extra Parameter beside Column name|\n",
    "|[5.2.3. Register UDF on the Fly](#5.2.3.-Register-UDF-on-the-Fly:)|Create Anonymous UDFs for DF API|\n",
    "|[5.3. Putting together everything in one place](#5.3.-Putting-together-everything-in-one-place:)|Concluding the Spark SQL use case|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting random seed for notebook reproducability\n",
    "rnd_seed=42\n",
    "np.random.seed=rnd_seed\n",
    "np.random.set_state=rnd_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Data Set:\n",
    "\n",
    "**restore_log_contents** - a collection of events, one per line, where each event is an instance when a file was resotred from backup.\n",
    "\n",
    "Following are two log lines from the file. Note all log lines are not of same length, some has more information than the others.\n",
    "\n",
    "```\n",
    "2018083015552003,AUDIT-LOG,Thu Aug 30 09:56:02 CEST 2018 RestoreTransfer[Aug 30 09:56:02]:616daf5c-ac23-11e8-946b-00a098de77e9 Operation-Uuid=23cfa2c4-ac2a-11e8-946b-00a098de77e9 Group=none Operation-Cookie=0 action=Start source=172.19.32.86:/share/afftest_testnfs destination=afftest:testnfs\n",
    "```\n",
    "\n",
    "```\n",
    "2018083100500289,AUDIT-LOG,Thu Aug 30 23:43:24 PDT 2018 RestoreTransfer[Aug 30 23:43:12]:1e74b7a8-ace9-11e8-8278-00a098aa1cdd Operation-Uuid=1e74b79b-ace9-11e8-8278-00a098aa1cdd Group=none Operation-Cookie=0 action=End source=MAN-PRD-FS:LHR_PRD_FS_LHR_PRD_FS_candidateRM_mirror_vault destination=MAN-PRD-FS:Gerry_LHR_PRD_FS1_candidateRM status=Success bytes_transferred=268838556 network_compression_ratio=1.0:1 transfer_desc=Logical Transfer with Storage Efficiency - Optimized Directory Mode\n",
    "```\n",
    "\n",
    "We are interested in finding the following sections from the log file. We will use regex extensivly to capture these patterns.\n",
    "\n",
    "![](assets/sections_of_interest.png)\n",
    "\n",
    "Please check [regex101](https://regex101.com/) to try out your regex before coding them.\n",
    "\n",
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following must be set in your .bashrc file\n",
    "#SPARK_HOME=\"/home/ubuntu/spark-2.4.0-bin-hadoop2.7\"\n",
    "#ANACONDA_HOME=\"/home/ubuntu/anaconda3/envs/pyspark\"\n",
    "#PYSPARK_PYTHON=\"$ANACONDA_HOME/bin/python\"\n",
    "#PYSPARK_DRIVER_PYTHON=\"$ANACONDA_HOME/bin/python\"\n",
    "#PYTHONPATH=\"$ANACONDA_HOME/bin/python\"\n",
    "#export PATH=\"$ANACONDA_HOME/bin:$SPARK_HOME/bin:$PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Work\\\\spark-2.3.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')\n",
    "         .appName('working-with-udfs')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MISHER:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>working-with-files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x211138b0f60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MISHER:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>working-with-files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=working-with-files>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x211138b8978>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Data From Files Into DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audit_log = spark.read.csv(path='data/restore_log_contents.csv', header=False, inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0             |_c1      |_c2                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+----------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:02 CEST 2018 RestoreTransfer[Aug 30 09:56:02]:616daf5c-ac23-11e8-946b-00a098de77e9 Operation-Uuid=23cfa2c4-ac2a-11e8-946b-00a098de77e9 Group=none Operation-Cookie=0 action=Start source=172.19.32.86:/share/afftest_testnfs destination=afftest:testnfs                                                                                                                                                                             |\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:02 CEST 2018 RestoreTransfer[Aug 30 09:56:02]:616daf5c-ac23-11e8-946b-00a098de77e9 Operation-Uuid=23cfa2c4-ac2a-11e8-946b-00a098de77e9 Group=none Operation-Cookie=0 action=End source=172.19.32.86:/share/afftest_testnfs destination=afftest:testnfs status=Success bytes_transferred=0 network_compression_ratio=1.0:1                                                                                                            |\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:10 PDT 2018 RestoreTransfer[Aug 30 13:44:10]:6f3486ed-ac95-11e8-ac6e-00a0982d695e Operation-Uuid=6f3486d7-ac95-11e8-ac6e-00a0982d695e Group=none Operation-Cookie=0 action=Start source=tiblab-fs1:restore1_vault destination=eaclab-fs1:tiblab_fs1_restore1_vault_restore                                                                                                                                                           |\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:22 PDT 2018 RestoreTransfer[Aug 30 13:44:10]:6f3486ed-ac95-11e8-ac6e-00a0982d695e Operation-Uuid=6f3486d7-ac95-11e8-ac6e-00a0982d695e Group=none Operation-Cookie=0 action=End source=tiblab-fs1:restore1_vault destination=eaclab-fs1:tiblab_fs1_restore1_vault_restore status=Success bytes_transferred=513743160 network_compression_ratio=1.0:1 transfer_desc=Logical Transfer with Storage Efficiency - Optimized Directory Mode|\n",
      "|2018083100500332|AUDIT-LOG|Thu Aug 30 23:40:12 PDT 2018 RestoreTransfer[Aug 30 23:40:12]:b2f4d3bc-ace8-11e8-8278-00a098aa1cdd Operation-Uuid=b2f4d3ae-ace8-11e8-8278-00a098aa1cdd Group=none Operation-Cookie=0 action=Start source=MAN-PRD-FS:LHR_PRD_FS1_LHR_PRD_FS1_3rdParty_mirror_vault destination=MAN-PRD-FS1:Gerry_LHR_PRD_FS1_3rdParty                                                                                                                                  |\n",
      "+----------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audit_log.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Fix the Headers and Provide a Schema:\n",
    "\n",
    "The Log file clearly lacks Headers and we did not provide any schema. Let's fix them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the schema, corresponding to a line in the csv data file for log\n",
    "log_schema = StructType([StructField('event_id', StringType(), nullable=False), \n",
    "                         StructField('log_type', StringType(), nullable=False),\n",
    "                         StructField('log_text', StringType(), nullable=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audit_log = spark.read.csv(path='data/restore_log_contents.csv', header=False, schema=log_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|event_id        |log_type |log_text                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+----------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:02 CEST 2018 RestoreTransfer[Aug 30 09:56:02]:616daf5c-ac23-11e8-946b-00a098de77e9 Operation-Uuid=23cfa2c4-ac2a-11e8-946b-00a098de77e9 Group=none Operation-Cookie=0 action=Start source=172.19.32.86:/share/afftest_testnfs destination=afftest:testnfs                                                                                                                                                                             |\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:02 CEST 2018 RestoreTransfer[Aug 30 09:56:02]:616daf5c-ac23-11e8-946b-00a098de77e9 Operation-Uuid=23cfa2c4-ac2a-11e8-946b-00a098de77e9 Group=none Operation-Cookie=0 action=End source=172.19.32.86:/share/afftest_testnfs destination=afftest:testnfs status=Success bytes_transferred=0 network_compression_ratio=1.0:1                                                                                                            |\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:10 PDT 2018 RestoreTransfer[Aug 30 13:44:10]:6f3486ed-ac95-11e8-ac6e-00a0982d695e Operation-Uuid=6f3486d7-ac95-11e8-ac6e-00a0982d695e Group=none Operation-Cookie=0 action=Start source=tiblab-fs1:restore1_vault destination=eaclab-fs1:tiblab_fs1_restore1_vault_restore                                                                                                                                                           |\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:22 PDT 2018 RestoreTransfer[Aug 30 13:44:10]:6f3486ed-ac95-11e8-ac6e-00a0982d695e Operation-Uuid=6f3486d7-ac95-11e8-ac6e-00a0982d695e Group=none Operation-Cookie=0 action=End source=tiblab-fs1:restore1_vault destination=eaclab-fs1:tiblab_fs1_restore1_vault_restore status=Success bytes_transferred=513743160 network_compression_ratio=1.0:1 transfer_desc=Logical Transfer with Storage Efficiency - Optimized Directory Mode|\n",
      "|2018083100500332|AUDIT-LOG|Thu Aug 30 23:40:12 PDT 2018 RestoreTransfer[Aug 30 23:40:12]:b2f4d3bc-ace8-11e8-8278-00a098aa1cdd Operation-Uuid=b2f4d3ae-ace8-11e8-8278-00a098aa1cdd Group=none Operation-Cookie=0 action=Start source=MAN-PRD-FS:LHR_PRD_FS1_LHR_PRD_FS1_3rdParty_mirror_vault destination=MAN-PRD-FS1:Gerry_LHR_PRD_FS1_3rdParty                                                                                                                                  |\n",
      "+----------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audit_log.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audit_log.createOrReplaceTempView('audit_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using UDFs with DF API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Extract Columns using In-built Regex Functions:\n",
    "\n",
    "To start with we will use the `regexp_extract` function from the `pyspark.sql.functions` package.\n",
    "\n",
    "**Note:** Notice the usage of `\\\\` in the regex expression. When you try at [regex101](https://regex101.com/) you do not need to give the two `\\\\`; one `\\` is enough. `\\\\` is provided to escape one `\\`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[event_id: string, log_type: string, log_text: string, log_time: string, event_time: string, relationship_id: string, operation_uuid: string, action: string, source: string, destination: string, status: string, bytes_transferred: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_log_inbuilt_regex = (audit_log\n",
    "            .withColumn('log_time', \n",
    "                F.regexp_extract('log_text', '^(\\\\w{3}\\\\s\\\\w{3}\\\\s\\\\d{2}\\\\s\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\s\\\\w{3,4}\\\\s\\\\d{4})', idx=1))\n",
    "            .withColumn('event_time', \n",
    "                F.regexp_extract('log_text', '\\\\[(\\\\w{3}\\\\s\\\\d{2}\\\\s\\\\d{2}:\\\\d{2}:\\\\d{2})]', idx=1))\n",
    "            .withColumn('relationship_id', \n",
    "                F.regexp_extract('log_text', '\\\\]\\\\:(.*?)\\\\s+', idx=1))\n",
    "            .withColumn('operation_uuid', \n",
    "                F.regexp_extract('log_text', 'Operation-Uuid\\\\=(.*?)\\\\s+', idx=1))\n",
    "            .withColumn('action', \n",
    "                F.regexp_extract('log_text', 'action\\\\=(.*?)\\\\s+', idx=1))\n",
    "            .withColumn('source', \n",
    "                F.regexp_extract('log_text', 'source\\\\=(.*?)\\\\s+', idx=1))\n",
    "            .withColumn('destination', \n",
    "                F.regexp_extract('log_text', 'destination\\\\=(.*?)(?=\\\\s|$)', idx=1))\n",
    "            .withColumn('status', \n",
    "                F.regexp_extract('log_text', 'status\\\\=(.*?)\\\\s+', idx=1))\n",
    "            .withColumn('bytes_transferred', \n",
    "                F.regexp_extract('log_text', 'bytes_transferred\\\\=(.*?)\\\\s+', idx=1))\n",
    "               )\n",
    "audit_log_inbuilt_regex.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>log_time</th>\n",
       "      <th>event_time</th>\n",
       "      <th>relationship_id</th>\n",
       "      <th>operation_uuid</th>\n",
       "      <th>action</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes_transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018083015552003</td>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Aug 30 09:56:02</td>\n",
       "      <td>616daf5c-ac23-11...</td>\n",
       "      <td>23cfa2c4-ac2a-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>172.19.32.86:/sh...</td>\n",
       "      <td>afftest:testnfs</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018083015552003</td>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Aug 30 09:56:02</td>\n",
       "      <td>616daf5c-ac23-11...</td>\n",
       "      <td>23cfa2c4-ac2a-11...</td>\n",
       "      <td>End</td>\n",
       "      <td>172.19.32.86:/sh...</td>\n",
       "      <td>afftest:testnfs</td>\n",
       "      <td>Success</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018083100220028</td>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Aug 30 13:44:10</td>\n",
       "      <td>6f3486ed-ac95-11...</td>\n",
       "      <td>6f3486d7-ac95-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>tiblab-fs1:resto...</td>\n",
       "      <td>eaclab-fs1:tibla...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018083100220028</td>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Aug 30 13:44:10</td>\n",
       "      <td>6f3486ed-ac95-11...</td>\n",
       "      <td>6f3486d7-ac95-11...</td>\n",
       "      <td>End</td>\n",
       "      <td>tiblab-fs1:resto...</td>\n",
       "      <td>eaclab-fs1:tibla...</td>\n",
       "      <td>Success</td>\n",
       "      <td>513743160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018083100500332</td>\n",
       "      <td>Thu Aug 30 23:40...</td>\n",
       "      <td>Aug 30 23:40:12</td>\n",
       "      <td>b2f4d3bc-ace8-11...</td>\n",
       "      <td>b2f4d3ae-ace8-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>MAN-PRD-FS:LHR_P...</td>\n",
       "      <td>MAN-PRD-FS1:Gerr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           event_id             log_time       event_time  \\\n",
       "0  2018083015552003  Thu Aug 30 09:56...  Aug 30 09:56:02   \n",
       "1  2018083015552003  Thu Aug 30 09:56...  Aug 30 09:56:02   \n",
       "2  2018083100220028  Thu Aug 30 13:44...  Aug 30 13:44:10   \n",
       "3  2018083100220028  Thu Aug 30 13:44...  Aug 30 13:44:10   \n",
       "4  2018083100500332  Thu Aug 30 23:40...  Aug 30 23:40:12   \n",
       "\n",
       "       relationship_id       operation_uuid action               source  \\\n",
       "0  616daf5c-ac23-11...  23cfa2c4-ac2a-11...  Start  172.19.32.86:/sh...   \n",
       "1  616daf5c-ac23-11...  23cfa2c4-ac2a-11...    End  172.19.32.86:/sh...   \n",
       "2  6f3486ed-ac95-11...  6f3486d7-ac95-11...  Start  tiblab-fs1:resto...   \n",
       "3  6f3486ed-ac95-11...  6f3486d7-ac95-11...    End  tiblab-fs1:resto...   \n",
       "4  b2f4d3bc-ace8-11...  b2f4d3ae-ace8-11...  Start  MAN-PRD-FS:LHR_P...   \n",
       "\n",
       "           destination   status bytes_transferred  \n",
       "0      afftest:testnfs                             \n",
       "1      afftest:testnfs  Success                 0  \n",
       "2  eaclab-fs1:tibla...                             \n",
       "3  eaclab-fs1:tibla...  Success         513743160  \n",
       "4  MAN-PRD-FS1:Gerr...                             "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(audit_log_inbuilt_regex\n",
    " .select('event_id', 'log_time', 'event_time', 'relationship_id', 'operation_uuid',  \n",
    "         'action', 'source', 'destination', 'status', 'bytes_transferred')\n",
    ".limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extract Columns using Custom Regex Functions:\n",
    "\n",
    "We will now develop our own custom functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Specific Extraction Functions:\n",
    "\n",
    "Custom function to extract the `log_time` section in the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_log_time(log):\n",
    "    pattern = re.compile('^(\\\\w{3}\\\\s\\\\w{3}\\\\s\\\\d{2}\\\\s\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\s\\\\w{3,4}\\\\s\\\\d{4})')\n",
    "    match = re.search(pattern, log)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thu Aug 30 09:56:02 CEST 2018'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's apply that to extract the log_time\n",
    "extract_log_time(\"Thu Aug 30 09:56:02 CEST 2018 RestoreTransfer[Aug 30 09:56:02]:616daf5c-ac23-11e8-946b-00a098de77e9 Operation-Uuid=23cfa2c4-ac2a-11e8-946b-00a098de77e9 Group=none Operation-Cookie=0 action=Start source=172.19.32.86:/share/afftest_testnfs destination=afftest:testnfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function to extract the `status` section in the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_status(log):\n",
    "    pattern = re.compile('status\\\\=(.*?)\\\\s+')\n",
    "    match = re.search(pattern, log)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's apply that to extract the status\n",
    "extract_status(\"Thu Aug 30 09:56:02 CEST 2018 RestoreTransfer[Aug 30 09:56:02]:616daf5c-ac23-11e8-946b-00a098de77e9 Operation-Uuid=23cfa2c4-ac2a-11e8-946b-00a098de77e9 Group=none Operation-Cookie=0 action=Start source=172.19.32.86:/share/afftest_testnfs destination=afftest:testnfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops!** What happened? Well the log line did not have the `status` section. Remember in the beginning I mentioned that not all sections are present in all log lines.\n",
    "\n",
    "Let's try another log line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's apply that to extract the status of another log line\n",
    "extract_status(\"Thu Aug 30 13:44:22 PDT 2018 RestoreTransfer[Aug 30 13:44:10]:6f3486ed-ac95-11e8-ac6e-00a0982d695e Operation-Uuid=6f3486d7-ac95-11e8-ac6e-00a0982d695e Group=none Operation-Cookie=0 action=End source=tiblab-fs1:restore1_vault destination=eaclab-fs1:tiblab_fs1_restore1_vault_restore status=Success bytes_transferred=513743160 network_compression_ratio=1.0:1 transfer_desc=Logical Transfer with Storage Efficiency - Optimized Directory Mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! This log line hasd the `status` section. It is important to note that not all sections will be present in all line, hence we added `if match` code block in our functions otherwise we would get exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Generic Extraction Functions:\n",
    "\n",
    "A generic function which takes the regular expression and the column name and applies the pattern on the column and extracts out the first match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_pattern(regex, log):\n",
    "    pattern = re.compile(regex)\n",
    "    match = re.search(pattern, log)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6f3486d7-ac95-11e8-ac6e-00a0982d695e'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's apply that to extract the operation_uuid\n",
    "extract_pattern('Operation-Uuid\\\\=(.*?)\\\\s+', \"Thu Aug 30 13:44:22 PDT 2018 RestoreTransfer[Aug 30 13:44:10]:6f3486ed-ac95-11e8-ac6e-00a0982d695e Operation-Uuid=6f3486d7-ac95-11e8-ac6e-00a0982d695e Group=none Operation-Cookie=0 action=End source=tiblab-fs1:restore1_vault destination=eaclab-fs1:tiblab_fs1_restore1_vault_restore status=Success bytes_transferred=513743160 network_compression_ratio=1.0:1 transfer_desc=Logical Transfer with Storage Efficiency - Optimized Directory Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's apply that to extract the status where the section is absent\n",
    "extract_pattern('status\\\\=(.*?)\\\\s+', \"Thu Aug 30 09:56:02 CEST 2018 RestoreTransfer[Aug 30 09:56:02]:616daf5c-ac23-11e8-946b-00a098de77e9 Operation-Uuid=23cfa2c4-ac2a-11e8-946b-00a098de77e9 Group=none Operation-Cookie=0 action=Start source=172.19.32.86:/share/afftest_testnfs destination=afftest:testnfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's apply that to extract the status of another log line\n",
    "extract_pattern('status\\\\=(.*?)\\\\s+', \"Thu Aug 30 13:44:22 PDT 2018 RestoreTransfer[Aug 30 13:44:10]:6f3486ed-ac95-11e8-ac6e-00a0982d695e Operation-Uuid=6f3486d7-ac95-11e8-ac6e-00a0982d695e Group=none Operation-Cookie=0 action=End source=tiblab-fs1:restore1_vault destination=eaclab-fs1:tiblab_fs1_restore1_vault_restore status=Success bytes_transferred=513743160 network_compression_ratio=1.0:1 transfer_desc=Logical Transfer with Storage Efficiency - Optimized Directory Mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Register the function as an UDF to be used with DF API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register extract_log_time as an UDF\n",
    "udf_extract_log_time = udf(extract_log_time, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register extract_status as an UDF\n",
    "udf_extract_status = udf(extract_status, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+--------------------+--------------------+-------+\n",
      "|        event_id| log_type|            log_text|            log_time| status|\n",
      "+----------------+---------+--------------------+--------------------+-------+\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|Thu Aug 30 09:56:...|   null|\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|Thu Aug 30 09:56:...|Success|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|Thu Aug 30 13:44:...|   null|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|Thu Aug 30 13:44:...|Success|\n",
      "|2018083100500332|AUDIT-LOG|Thu Aug 30 23:40:...|Thu Aug 30 23:40:...|   null|\n",
      "+----------------+---------+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the above UDFs to extract the fields\n",
    "(audit_log\n",
    " .withColumn('log_time', udf_extract_log_time('log_text'))\n",
    " .withColumn('status', udf_extract_status('log_text'))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4. Curry up function that takes more than One Parameter:\n",
    "\n",
    "**Unfortunately PySpark DataFrame udfs take only col names as parameters and nothing else.** So we cannot use `extract_pattern` as `udf(extract_pattern, StringType())` and pass the reqex pattern as well as the column name in the DF API calls. We need to curry the function, so that the only argument in the DataFrame call is the name of the column on which we want the function to act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a wrapper around extract_pattern to be used as an UDF in DF API\n",
    "event_time_pattern = '\\\\[(\\\\w{3}\\\\s\\\\d{2}\\\\s\\\\d{2}:\\\\d{2}:\\\\d{2})]'\n",
    "udf_extract_event_time = udf(lambda log_text : extract_pattern(event_time_pattern, log_text), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+--------------------+---------------+\n",
      "|        event_id| log_type|            log_text|     event_time|\n",
      "+----------------+---------+--------------------+---------------+\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|Aug 30 09:56:02|\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|Aug 30 09:56:02|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|Aug 30 13:44:10|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|Aug 30 13:44:10|\n",
      "|2018083100500332|AUDIT-LOG|Thu Aug 30 23:40:...|Aug 30 23:40:12|\n",
      "+----------------+---------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(audit_log\n",
    "  .withColumn('event_time', udf_extract_event_time('log_text'))\n",
    " ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5. Register UDF on the Fly:\n",
    "\n",
    "We can also use our custom function as an UDF by creating an UDF on the fly. If we want to use the custom function over and over again with different parameters then creating new UDF functions for each different parameter might be cumbersome. In that case we can register our function as an UDF on the fly and pass different parameters as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+--------------------+--------------------+\n",
      "|        event_id| log_type|            log_text|     relationship_id|\n",
      "+----------------+---------+--------------------+--------------------+\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|616daf5c-ac23-11e...|\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|616daf5c-ac23-11e...|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|6f3486ed-ac95-11e...|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|6f3486ed-ac95-11e...|\n",
      "|2018083100500332|AUDIT-LOG|Thu Aug 30 23:40:...|b2f4d3bc-ace8-11e...|\n",
      "+----------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(audit_log\n",
    "  .withColumn('relationship_id', udf(lambda log : extract_pattern('\\\\]\\\\:(.*?)\\\\s+', log), StringType())('log_text'))\n",
    " ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.6. Hack PySpark Codes to call Scala functions directly:\n",
    "\n",
    "If we are still unhappy with the way we have to curry up our Pyspark UDFs we can just look into the source code of PySpark and hack up our own methods.\n",
    "\n",
    "Here I just hacked the PySpark's `regexp_extract` and created my own `custom_regexp_extract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.column import Column, _to_java_column, _to_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_regexp_extract(str, pattern, idx):\n",
    "    sc = SparkContext._active_spark_context\n",
    "    jc = sc._jvm.functions.regexp_extract(_to_java_column(str), pattern, idx)\n",
    "    return Column(jc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+--------------------+--------------------+\n",
      "|        event_id| log_type|            log_text|      operation_uuid|\n",
      "+----------------+---------+--------------------+--------------------+\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|23cfa2c4-ac2a-11e...|\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|23cfa2c4-ac2a-11e...|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|6f3486d7-ac95-11e...|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|6f3486d7-ac95-11e...|\n",
      "|2018083100500332|AUDIT-LOG|Thu Aug 30 23:40:...|b2f4d3ae-ace8-11e...|\n",
      "+----------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(audit_log\n",
    "  .withColumn('operation_uuid', custom_regexp_extract('log_text', 'Operation-Uuid\\\\=(.*?)\\\\s+', 1))\n",
    " ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Putting together everything in one place:\n",
    "\n",
    "Here we show all the 4 types of custom UDF that we have discussed so far along with inbuilt UDFs being implemented in DF API call  and how they work seamless with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audit_log_custom_regex = (audit_log\n",
    "            .withColumn('log_time', udf_extract_log_time('log_text'))\n",
    "            .withColumn('event_time', udf_extract_event_time('log_text'))\n",
    "            .withColumn('relationship_id', udf(lambda log : extract_pattern('\\\\]\\\\:(.*?)\\\\s+', log), StringType())('log_text'))\n",
    "            .withColumn('operation_uuid', custom_regexp_extract('log_text', 'Operation-Uuid\\\\=(.*?)\\\\s+', 1))\n",
    "            .withColumn('action', \n",
    "                F.regexp_extract('log_text', 'action\\\\=(.*?)\\\\s+', idx=1))\n",
    "            .withColumn('source', custom_regexp_extract('log_text', 'source\\\\=(.*?)\\\\s+', 1))\n",
    "            .withColumn('destination', udf(lambda log : extract_pattern('destination\\\\=(.*?)(?=\\\\s|$)', log), StringType())('log_text'))\n",
    "            .withColumn('status', \n",
    "                F.regexp_extract('log_text', 'status\\\\=(.*?)\\\\s+', idx=1))\n",
    "            .withColumn('bytes_transferred', \n",
    "                F.regexp_extract('log_text', 'bytes_transferred\\\\=(.*?)\\\\s+', idx=1))\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[event_id: string, log_type: string, log_text: string, log_time: string, event_time: string, relationship_id: string, operation_uuid: string, action: string, source: string, destination: string, status: string, bytes_transferred: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_log_inbuilt_regex.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>log_type</th>\n",
       "      <th>log_text</th>\n",
       "      <th>log_time</th>\n",
       "      <th>event_time</th>\n",
       "      <th>relationship_id</th>\n",
       "      <th>operation_uuid</th>\n",
       "      <th>action</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes_transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018083015552003</td>\n",
       "      <td>AUDIT-LOG</td>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Aug 30 09:56:02</td>\n",
       "      <td>616daf5c-ac23-11...</td>\n",
       "      <td>23cfa2c4-ac2a-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>172.19.32.86:/sh...</td>\n",
       "      <td>afftest:testnfs</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018083015552003</td>\n",
       "      <td>AUDIT-LOG</td>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Aug 30 09:56:02</td>\n",
       "      <td>616daf5c-ac23-11...</td>\n",
       "      <td>23cfa2c4-ac2a-11...</td>\n",
       "      <td>End</td>\n",
       "      <td>172.19.32.86:/sh...</td>\n",
       "      <td>afftest:testnfs</td>\n",
       "      <td>Success</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018083100220028</td>\n",
       "      <td>AUDIT-LOG</td>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Aug 30 13:44:10</td>\n",
       "      <td>6f3486ed-ac95-11...</td>\n",
       "      <td>6f3486d7-ac95-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>tiblab-fs1:resto...</td>\n",
       "      <td>eaclab-fs1:tibla...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018083100220028</td>\n",
       "      <td>AUDIT-LOG</td>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Aug 30 13:44:10</td>\n",
       "      <td>6f3486ed-ac95-11...</td>\n",
       "      <td>6f3486d7-ac95-11...</td>\n",
       "      <td>End</td>\n",
       "      <td>tiblab-fs1:resto...</td>\n",
       "      <td>eaclab-fs1:tibla...</td>\n",
       "      <td>Success</td>\n",
       "      <td>513743160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018083100500332</td>\n",
       "      <td>AUDIT-LOG</td>\n",
       "      <td>Thu Aug 30 23:40...</td>\n",
       "      <td>Thu Aug 30 23:40...</td>\n",
       "      <td>Aug 30 23:40:12</td>\n",
       "      <td>b2f4d3bc-ace8-11...</td>\n",
       "      <td>b2f4d3ae-ace8-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>MAN-PRD-FS:LHR_P...</td>\n",
       "      <td>MAN-PRD-FS1:Gerr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           event_id   log_type             log_text             log_time  \\\n",
       "0  2018083015552003  AUDIT-LOG  Thu Aug 30 09:56...  Thu Aug 30 09:56...   \n",
       "1  2018083015552003  AUDIT-LOG  Thu Aug 30 09:56...  Thu Aug 30 09:56...   \n",
       "2  2018083100220028  AUDIT-LOG  Thu Aug 30 13:44...  Thu Aug 30 13:44...   \n",
       "3  2018083100220028  AUDIT-LOG  Thu Aug 30 13:44...  Thu Aug 30 13:44...   \n",
       "4  2018083100500332  AUDIT-LOG  Thu Aug 30 23:40...  Thu Aug 30 23:40...   \n",
       "\n",
       "        event_time      relationship_id       operation_uuid action  \\\n",
       "0  Aug 30 09:56:02  616daf5c-ac23-11...  23cfa2c4-ac2a-11...  Start   \n",
       "1  Aug 30 09:56:02  616daf5c-ac23-11...  23cfa2c4-ac2a-11...    End   \n",
       "2  Aug 30 13:44:10  6f3486ed-ac95-11...  6f3486d7-ac95-11...  Start   \n",
       "3  Aug 30 13:44:10  6f3486ed-ac95-11...  6f3486d7-ac95-11...    End   \n",
       "4  Aug 30 23:40:12  b2f4d3bc-ace8-11...  b2f4d3ae-ace8-11...  Start   \n",
       "\n",
       "                source          destination   status bytes_transferred  \n",
       "0  172.19.32.86:/sh...      afftest:testnfs                             \n",
       "1  172.19.32.86:/sh...      afftest:testnfs  Success                 0  \n",
       "2  tiblab-fs1:resto...  eaclab-fs1:tibla...                             \n",
       "3  tiblab-fs1:resto...  eaclab-fs1:tibla...  Success         513743160  \n",
       "4  MAN-PRD-FS:LHR_P...  MAN-PRD-FS1:Gerr...                             "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_log_custom_regex.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using UDFs with PySpark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+--------------------+\n",
      "|        event_id| log_type|            log_text|\n",
      "+----------------+---------+--------------------+\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|\n",
      "|2018083015552003|AUDIT-LOG|Thu Aug 30 09:56:...|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|\n",
      "|2018083100220028|AUDIT-LOG|Thu Aug 30 13:44:...|\n",
      "|2018083100500332|AUDIT-LOG|Thu Aug 30 23:40:...|\n",
      "+----------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from audit_log\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Extract Columns using In-built Regex Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same regex expressions and the inbuilt `regexp_extract` function to extract the fields from the `log_text` column. \n",
    "\n",
    "**Note:** However we now need to use three `\\\\\\` in the SQL for regex expressions instead of two `\\\\` as in the DF API. When you try at [regex101](https://regex101.com/) you do not need to give the two `\\\\`; one `\\` is enough; `\\\\` is provided to escape one `\\`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_time</th>\n",
       "      <th>event_time</th>\n",
       "      <th>relationship_id</th>\n",
       "      <th>operation_uuid</th>\n",
       "      <th>action</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes_transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Aug 30 09:56:02</td>\n",
       "      <td>616daf5c-ac23-11...</td>\n",
       "      <td>23cfa2c4-ac2a-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>172.19.32.86:/sh...</td>\n",
       "      <td>afftest:testnfs</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Aug 30 09:56:02</td>\n",
       "      <td>616daf5c-ac23-11...</td>\n",
       "      <td>23cfa2c4-ac2a-11...</td>\n",
       "      <td>End</td>\n",
       "      <td>172.19.32.86:/sh...</td>\n",
       "      <td>afftest:testnfs</td>\n",
       "      <td>Success</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Aug 30 13:44:10</td>\n",
       "      <td>6f3486ed-ac95-11...</td>\n",
       "      <td>6f3486d7-ac95-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>tiblab-fs1:resto...</td>\n",
       "      <td>eaclab-fs1:tibla...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Aug 30 13:44:10</td>\n",
       "      <td>6f3486ed-ac95-11...</td>\n",
       "      <td>6f3486d7-ac95-11...</td>\n",
       "      <td>End</td>\n",
       "      <td>tiblab-fs1:resto...</td>\n",
       "      <td>eaclab-fs1:tibla...</td>\n",
       "      <td>Success</td>\n",
       "      <td>513743160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Aug 30 23:40...</td>\n",
       "      <td>Aug 30 23:40:12</td>\n",
       "      <td>b2f4d3bc-ace8-11...</td>\n",
       "      <td>b2f4d3ae-ace8-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>MAN-PRD-FS:LHR_P...</td>\n",
       "      <td>MAN-PRD-FS1:Gerr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              log_time       event_time      relationship_id  \\\n",
       "0  Thu Aug 30 09:56...  Aug 30 09:56:02  616daf5c-ac23-11...   \n",
       "1  Thu Aug 30 09:56...  Aug 30 09:56:02  616daf5c-ac23-11...   \n",
       "2  Thu Aug 30 13:44...  Aug 30 13:44:10  6f3486ed-ac95-11...   \n",
       "3  Thu Aug 30 13:44...  Aug 30 13:44:10  6f3486ed-ac95-11...   \n",
       "4  Thu Aug 30 23:40...  Aug 30 23:40:12  b2f4d3bc-ace8-11...   \n",
       "\n",
       "        operation_uuid action               source          destination  \\\n",
       "0  23cfa2c4-ac2a-11...  Start  172.19.32.86:/sh...      afftest:testnfs   \n",
       "1  23cfa2c4-ac2a-11...    End  172.19.32.86:/sh...      afftest:testnfs   \n",
       "2  6f3486d7-ac95-11...  Start  tiblab-fs1:resto...  eaclab-fs1:tibla...   \n",
       "3  6f3486d7-ac95-11...    End  tiblab-fs1:resto...  eaclab-fs1:tibla...   \n",
       "4  b2f4d3ae-ace8-11...  Start  MAN-PRD-FS:LHR_P...  MAN-PRD-FS1:Gerr...   \n",
       "\n",
       "    status bytes_transferred  \n",
       "0                             \n",
       "1  Success                 0  \n",
       "2                             \n",
       "3  Success         513743160  \n",
       "4                             "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT \n",
    "regexp_extract(log_text, '^(\\\\\\w{3}\\\\\\s\\\\\\w{3}\\\\\\s\\\\\\d{2}\\\\\\s\\\\\\d{2}:\\\\\\d{2}:\\\\\\d{2}\\\\\\s\\\\\\w{3,4}\\\\\\s\\\\\\d{4})', 1) as log_time,\n",
    "regexp_extract(log_text, '\\\\\\[(\\\\\\w{3}\\\\\\s\\\\\\d{2}\\\\\\s\\\\\\d{2}:\\\\\\d{2}:\\\\\\d{2})]', 1) as event_time,\n",
    "regexp_extract(log_text, '\\\\\\]\\\\\\:(.*?)\\\\\\s+', 1) as relationship_id,\n",
    "regexp_extract(log_text, 'Operation-Uuid\\\\\\=(.*?)\\\\\\s+', 1) as operation_uuid,\n",
    "regexp_extract(log_text, 'action\\\\\\=(.*?)\\\\\\s+', 1) as action,\n",
    "regexp_extract(log_text, 'source\\\\\\=(.*?)\\\\\\s+', 1) as source,\n",
    "regexp_extract(log_text, 'destination\\\\\\=(.*?)(?=\\\\\\s|$)', 1) as destination,\n",
    "regexp_extract(log_text, 'status\\\\\\=(.*?)\\\\\\s+', 1) as status,\n",
    "regexp_extract(log_text, 'bytes_transferred\\\\\\=(.*?)\\\\\\s+', 1) as bytes_transferred\n",
    "FROM audit_log\n",
    "\"\"\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Extract Columns using Custom Regex Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1. Register the function to the SQLContext as an udf to be used within SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_log_time>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register extract_log_time as an UDF with Spark SQLContext\n",
    "spark.udf.register('udf_extract_log_time', extract_log_time, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_status>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register extract_status as an UDF with Spark SQLContext\n",
    "spark.udf.register('udf_extract_status', extract_status, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------+\n",
      "|log_time                     |status |\n",
      "+-----------------------------+-------+\n",
      "|Thu Aug 30 09:56:02 CEST 2018|null   |\n",
      "|Thu Aug 30 09:56:02 CEST 2018|Success|\n",
      "|Thu Aug 30 13:44:10 PDT 2018 |null   |\n",
      "|Thu Aug 30 13:44:22 PDT 2018 |Success|\n",
      "|Thu Aug 30 23:40:12 PDT 2018 |null   |\n",
      "+-----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the above UDFs to extract the fields\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT \n",
    "udf_extract_log_time(log_text) as log_time,\n",
    "udf_extract_status(log_text) as status\n",
    "FROM audit_log\n",
    "\"\"\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Function that takes more than One Parameter:\n",
    "\n",
    "**Fortunately PySpark SQL UDFs can take additional parameters along with column name.** So we can use `extract_pattern` as `spark.udf.register('udf_extract_pattern', extract_pattern, StringType())` and pass the reqex pattern as well as the column name in the SQL calls. We do need to curry the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_pattern>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the genericc extract_pattern function as an UDF with Spark SQLContext\n",
    "spark.udf.register('udf_extract_pattern', extract_pattern, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|event_time     |\n",
      "+---------------+\n",
      "|Aug 30 09:56:02|\n",
      "|Aug 30 09:56:02|\n",
      "|Aug 30 13:44:10|\n",
      "|Aug 30 13:44:10|\n",
      "|Aug 30 23:40:12|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT \n",
    "udf_extract_pattern('\\\\\\[(\\\\\\w{3}\\\\\\s\\\\\\d{2}\\\\\\s\\\\\\d{2}:\\\\\\d{2}:\\\\\\d{2})]', log_text) as event_time\n",
    "FROM audit_log\n",
    "\"\"\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3. Register UDF on the Fly:\n",
    "\n",
    "We can also use our custom function as an UDF by creating an UDF on the fly. If we want to use the custom function over and over again with different parameters then creating new UDF functions for each different parameter might be cumbersome. In that case we can register our function as an UDF on the fly and pass different parameters as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('udf_relationship_id', lambda log : extract_pattern('\\\\]\\\\:(.*?)\\\\s+', log), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|relationship_id                     |\n",
      "+------------------------------------+\n",
      "|616daf5c-ac23-11e8-946b-00a098de77e9|\n",
      "|616daf5c-ac23-11e8-946b-00a098de77e9|\n",
      "|6f3486ed-ac95-11e8-ac6e-00a0982d695e|\n",
      "|6f3486ed-ac95-11e8-ac6e-00a0982d695e|\n",
      "|b2f4d3bc-ace8-11e8-8278-00a098aa1cdd|\n",
      "+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT udf_relationship_id(log_text) as relationship_id\n",
    "FROM audit_log\n",
    "\"\"\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Putting together everything in one place:\n",
    "\n",
    "Here we show all the 3 types of custom UDF that we have discussed so far along with inbuilt UDFs being implemented in one query and how they work seamless with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_time</th>\n",
       "      <th>event_time</th>\n",
       "      <th>relationship_id</th>\n",
       "      <th>operation_uuid</th>\n",
       "      <th>action</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes_transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Aug 30 09:56:02</td>\n",
       "      <td>616daf5c-ac23-11...</td>\n",
       "      <td>23cfa2c4-ac2a-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>172.19.32.86:/sh...</td>\n",
       "      <td>afftest:testnfs</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Aug 30 09:56...</td>\n",
       "      <td>Aug 30 09:56:02</td>\n",
       "      <td>616daf5c-ac23-11...</td>\n",
       "      <td>23cfa2c4-ac2a-11...</td>\n",
       "      <td>End</td>\n",
       "      <td>172.19.32.86:/sh...</td>\n",
       "      <td>afftest:testnfs</td>\n",
       "      <td>Success</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Aug 30 13:44:10</td>\n",
       "      <td>6f3486ed-ac95-11...</td>\n",
       "      <td>6f3486d7-ac95-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>tiblab-fs1:resto...</td>\n",
       "      <td>eaclab-fs1:tibla...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Aug 30 13:44...</td>\n",
       "      <td>Aug 30 13:44:10</td>\n",
       "      <td>6f3486ed-ac95-11...</td>\n",
       "      <td>6f3486d7-ac95-11...</td>\n",
       "      <td>End</td>\n",
       "      <td>tiblab-fs1:resto...</td>\n",
       "      <td>eaclab-fs1:tibla...</td>\n",
       "      <td>Success</td>\n",
       "      <td>513743160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Aug 30 23:40...</td>\n",
       "      <td>Aug 30 23:40:12</td>\n",
       "      <td>b2f4d3bc-ace8-11...</td>\n",
       "      <td>b2f4d3ae-ace8-11...</td>\n",
       "      <td>Start</td>\n",
       "      <td>MAN-PRD-FS:LHR_P...</td>\n",
       "      <td>MAN-PRD-FS1:Gerr...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              log_time       event_time      relationship_id  \\\n",
       "0  Thu Aug 30 09:56...  Aug 30 09:56:02  616daf5c-ac23-11...   \n",
       "1  Thu Aug 30 09:56...  Aug 30 09:56:02  616daf5c-ac23-11...   \n",
       "2  Thu Aug 30 13:44...  Aug 30 13:44:10  6f3486ed-ac95-11...   \n",
       "3  Thu Aug 30 13:44...  Aug 30 13:44:10  6f3486ed-ac95-11...   \n",
       "4  Thu Aug 30 23:40...  Aug 30 23:40:12  b2f4d3bc-ace8-11...   \n",
       "\n",
       "        operation_uuid action               source          destination  \\\n",
       "0  23cfa2c4-ac2a-11...  Start  172.19.32.86:/sh...      afftest:testnfs   \n",
       "1  23cfa2c4-ac2a-11...    End  172.19.32.86:/sh...      afftest:testnfs   \n",
       "2  6f3486d7-ac95-11...  Start  tiblab-fs1:resto...  eaclab-fs1:tibla...   \n",
       "3  6f3486d7-ac95-11...    End  tiblab-fs1:resto...  eaclab-fs1:tibla...   \n",
       "4  b2f4d3ae-ace8-11...  Start  MAN-PRD-FS:LHR_P...  MAN-PRD-FS1:Gerr...   \n",
       "\n",
       "    status bytes_transferred  \n",
       "0     None                    \n",
       "1  Success                 0  \n",
       "2     None                    \n",
       "3  Success         513743160  \n",
       "4     None                    "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT \n",
    "udf_extract_log_time(log_text) as log_time,\n",
    "udf_extract_pattern('\\\\\\[(\\\\\\w{3}\\\\\\s\\\\\\d{2}\\\\\\s\\\\\\d{2}:\\\\\\d{2}:\\\\\\d{2})]', log_text) as event_time,\n",
    "udf_relationship_id(log_text) as relationship_id,\n",
    "regexp_extract(log_text, 'Operation-Uuid\\\\\\=(.*?)\\\\\\s+', 1) as operation_uuid,\n",
    "regexp_extract(log_text, 'action\\\\\\=(.*?)\\\\\\s+', 1) as action,\n",
    "udf_extract_pattern('source\\\\\\=(.*?)\\\\\\s+', log_text) as source,\n",
    "regexp_extract(log_text, 'destination\\\\\\=(.*?)(?=\\\\\\s|$)', 1) as destination,\n",
    "udf_extract_status(log_text) as status,\n",
    "regexp_extract(log_text, 'bytes_transferred\\\\\\=(.*?)\\\\\\s+', 1) as bytes_transferred\n",
    "FROM audit_log\n",
    "\"\"\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
