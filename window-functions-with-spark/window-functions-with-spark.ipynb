{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Window Functions in PySPark\n",
    "\n",
    "Window Functions helps us to compare current row with other rows in the same dataframe, calculating running totals, sequencing of events and sessionization of transactions etc. This notebook is compiled from the following blog posts and my additional insights and examples from my own understanding and day to day work:\n",
    "\n",
    "+ https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html \n",
    "+ http://alvinhenrick.com/2017/05/16/apache-spark-analytical-window-functions/ \n",
    "+ https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html \n",
    "+ https://johnpaton.net/posts/forward-fill-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "A brief synopsis of what each use case is and what functionality of the SPARK SQL or DF API does it touch on.\n",
    "\n",
    "| Section                                                                             |        Demonstrates |\n",
    "|:------------------------------------------------------------------------------------|:--------------------|\n",
    "|[1.1 Rank salary within each department](#1.1-Rank-salary-within-each-department:)|WINDOW, RANK|\n",
    "|[1.2 Dense Rank salary within each department](#1.2-Dense-Rank-salary-within-each-department:)|WINDOW, DENSE_RANK|\n",
    "|[1.3 Row Number within each department](#1.3-Row-Number-within-each-department:)|WINDOW, ROW_NUMBER|\n",
    "|[1.4 Running Total Salary within each department](#1.4-Running-Total-Salary-within-each-department:)|SUM, WINDOW|\n",
    "|[1.5 Next Salary within each department](#1.5-Next-Salary-within-each-department:)|LEAD, WINDOW|\n",
    "|[1.6 Previous Salary within each department](#1.6-Previous-Salary-within-each-department:)|LAG, WINDOW|\n",
    "|[1.7 First Salary within each department](#1.7-First-Salary-within-each-department:)|FIRST, WINDOW|\n",
    "|[1.8 Last Salary within each department](#1.8-Last-Salary-within-each-department:)|LAST, WINDOW|\n",
    "|[2.1 What are the best-selling and the second best-selling products in every category?](#2.1-What-are-the-best-selling-and-the-second-best-selling-products-in-every-category?)|WINDOW, DENSE_RANK, INNER SUB-QUERY|\n",
    "|[2.2 What is the difference between the revenue of each product and the revenue of the best selling product in the same category as that product?](#2.2-What-is-the-difference-between-the-revenue-of-each-product-and-the-revenue-of-the-best-selling-product-in-the-same-category-as-that-product?)|Highlights the Difference of ROWS BETWEEN and RANGE BETWEEN of WINDOW function|\n",
    "|[3.1 What is the first Purchase Date for each customer](#3.1-What-is-the-first-Purchase-Date-for-each-customer:)|FIRST, WINDOW, INNER SUB-QUERY|\n",
    "|[3.2 What is the cumulative amount spent by each customer](#3.2-What-is-the-cumulative-amount-spent-by-each-customer:)|SUM, WINDOW ROWS BETWEEN|\n",
    "|[3.3 What is the moving average between 3 consecutive transactions [previous, current, next] for each customer](#3.3-What-is-the-moving-average-between-3-consecutive-transactions-[previous,-current,-next]-for-each-customer:)|SUM, WINDOW with specific PRECEDING AND FOLLOWING values for ROWS BETWEEN|\n",
    "|[3.4 What is the 3 days moving average for each customer](#3.4-What-is-the-3-days-moving-average-for-each-customer:)|SUM, WINDOW with specific PRECEDING AND FOLLOWING values for RANGE BETWEEN|\n",
    "|[4.1 Running Difference of Current Sale from the Previous Sale](#4.1-Running-Difference-of-Current-Sale-from-the-Previous-Sale:)|LAG, WINDOW|\n",
    "|[5.1 Forward-fill and Backward-fill missing data in Spark](#5.1-Forward-fill-and-Backward-fill-missing-data-in-Spark:)|FIRST, LAST, WINDOW|\n",
    "|[5.2 Forward-fill and Backward-fill missing data without grouping](#5.2-Forward-fill-and-Backward-fill-missing-data-without-grouping:)|FIRST, LAST, WINDOW with no PARTITION BY|\n",
    "|[5.3 Forward-fill and Backward-fill missing data with a threshold](#5.3-Forward-fill-and-Backward-fill-missing-data-with-a-threshold:)|FIRST, LAST, WINDOW with no specific ROWS BETWEEN for PRECEDING and FOLLOWING|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting random seed for notebook reproducability\n",
    "rnd_seed=23\n",
    "np.random.seed=23\n",
    "np.random.set_state=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following must be set in your .bashrc file\n",
    "#SPARK_HOME=\"/home/ubuntu/spark-2.4.0-bin-hadoop2.7\"\n",
    "#ANACONDA_HOME=\"/home/ubuntu/anaconda3/envs/pyspark\"\n",
    "#PYSPARK_PYTHON=\"$ANACONDA_HOME/bin/python\"\n",
    "#PYSPARK_DRIVER_PYTHON=\"$ANACONDA_HOME/bin/python\"\n",
    "#PYTHONPATH=\"$ANACONDA_HOME/bin/python\"\n",
    "#export PATH=\"$ANACONDA_HOME/bin:$SPARK_HOME/bin:$PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"window-functions-with-pyspark\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-30-2-158.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>window-functions-with-pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc726cec5f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-30-2-158.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>window-functions-with-pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=window-functions-with-pyspark>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7fc7457dec50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Utility function to emulate stripMargin in Scala string.\n",
    "def strip_margin(text):\n",
    "    nomargin = re.sub('\\n[ \\t]*\\|', ' ', text)\n",
    "    trimmed = re.sub('\\s+', ' ', nomargin)\n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Employee Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Sample Dataframe\n",
    "employees = [\n",
    "    (7369, \"SMITH\", \"CLERK\", 7902, \"17-Dec-80\", 800, 20, 10),\n",
    "    (7499, \"ALLEN\", \"SALESMAN\", 7698, \"20-Feb-81\", 1600, 300, 30),\n",
    "    (7521, \"WARD\", \"SALESMAN\", 7698, \"22-Feb-81\", 1250, 500, 30),\n",
    "    (7566, \"JONES\", \"MANAGER\", 7839, \"2-Apr-81\", 2975, 0, 20),\n",
    "    (7654, \"MARTIN\", \"SALESMAN\", 7698, \"28-Sep-81\", 1250, 1400, 30),\n",
    "    (7698, \"BLAKE\", \"MANAGER\", 7839, \"1-May-81\", 2850, 0, 30),\n",
    "    (7782, \"CLARK\", \"MANAGER\", 7839, \"9-Jun-81\", 2450, 0, 10),\n",
    "    (7788, \"SCOTT\", \"ANALYST\", 7566, \"19-Apr-87\", 3000, 0, 20),\n",
    "    (7629, \"ALEX\", \"SALESMAN\", 7698, \"28-Sep-79\", 1150, 1400, 30),\n",
    "    (7839, \"KING\", \"PRESIDENT\", 0, \"17-Nov-81\", 5000, 0, 10),\n",
    "    (7844, \"TURNER\", \"SALESMAN\", 7698, \"8-Sep-81\", 1500, 0, 30),\n",
    "    (7876, \"ADAMS\", \"CLERK\", 7788, \"23-May-87\", 1100, 0, 20)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(employees, [\"empno\", \"ename\", \"job\", \"mgr\", \"hiredate\", \"sal\", \"comm\", \"deptno\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+---------+----+----+------+\n",
      "|empno| ename|      job| mgr| hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+---------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-Dec-80| 800|  20|    10|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-Feb-81|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-Feb-81|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839| 2-Apr-81|2975|   0|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-Sep-81|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839| 1-May-81|2850|   0|    30|\n",
      "| 7782| CLARK|  MANAGER|7839| 9-Jun-81|2450|   0|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-Apr-87|3000|   0|    20|\n",
      "| 7629|  ALEX| SALESMAN|7698|28-Sep-79|1150|1400|    30|\n",
      "| 7839|  KING|PRESIDENT|   0|17-Nov-81|5000|   0|    10|\n",
      "| 7844|TURNER| SALESMAN|7698| 8-Sep-81|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|23-May-87|1100|   0|    20|\n",
      "+-----+------+---------+----+---------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: long (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: long (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: long (nullable = true)\n",
      " |-- comm: long (nullable = true)\n",
      " |-- deptno: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Rank salary within each department:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using rank() Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----+\n",
      "|empno| ename|      job|deptno| sal|rank|\n",
      "+-----+------+---------+------+----+----+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|   1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|   2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|   3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|   1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|   2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|   3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|   4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|   4|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|   6|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|   1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|   2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|   3|\n",
      "+-----+------+---------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, RANK() OVER (PARTITION BY deptno ORDER BY sal DESC) AS rank\n",
    "          |FROM emp\n",
    "        \"\"\")).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using Window Function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will need to define the window we will be working on i.e. we will partition by department (deptno) and order by salary (sal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----+\n",
      "|empno| ename|      job|deptno| sal|rank|\n",
      "+-----+------+---------+------+----+----+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|   1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|   2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|   3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|   1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|   2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|   3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|   4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|   4|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|   6|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|   1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|   2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|   3|\n",
      "+-----+------+---------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.rank().over(windowSpec).alias('rank')).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dense Rank salary within each department:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using dense_rank() Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----------+\n",
      "|empno| ename|      job|deptno| sal|dense_rank|\n",
      "+-----+------+---------+------+----+----------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|         1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|         2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|         3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|         1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|         2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|         3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|         4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|         4|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|         5|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|         1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|         2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|         3|\n",
      "+-----+------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, DENSE_RANK() OVER (PARTITION BY deptno ORDER BY sal DESC) AS dense_rank\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----------+\n",
      "|empno| ename|      job|deptno| sal|dense_rank|\n",
      "+-----+------+---------+------+----+----------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|         1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|         2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|         3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|         1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|         2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|         3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|         4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|         4|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|         5|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|         1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|         2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|         3|\n",
      "+-----+------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.dense_rank().over(windowSpec).alias('dense_rank')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Row Number within each department:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using row_num() Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+-------+\n",
      "|empno| ename|      job|deptno| sal|row_num|\n",
      "+-----+------+---------+------+----+-------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|      1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|      2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|      3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|      1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|      2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|      3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|      4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|      5|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|      6|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|      1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|      2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|      3|\n",
      "+-----+------+---------+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, ROW_NUMBER() OVER (PARTITION BY deptno ORDER BY sal DESC) AS row_num\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+-------+\n",
      "|empno| ename|      job|deptno| sal|row_num|\n",
      "+-----+------+---------+------+----+-------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|      1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|      2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|      3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|      1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|      2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|      3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|      4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|      5|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|      6|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|      1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|      2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|      3|\n",
      "+-----+------+---------+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.row_number().over(windowSpec).alias('row_num')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Running Total Salary within each department:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using sum() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+-------------+\n",
      "|empno| ename|      job|deptno| sal|running_total|\n",
      "+-----+------+---------+------+----+-------------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|         5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|         7450|\n",
      "| 7369| SMITH|    CLERK|    10| 800|         8250|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|         2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|         4450|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|         5950|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|         8450|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|         8450|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|         9600|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|         3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|         5975|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|         7075|\n",
      "+-----+------+---------+------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, SUM(sal) OVER (PARTITION BY deptno ORDER BY sal DESC) AS running_total\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using sum() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+-------------+\n",
      "|empno| ename|      job|deptno| sal|running_total|\n",
      "+-----+------+---------+------+----+-------------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|         5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|         7450|\n",
      "| 7369| SMITH|    CLERK|    10| 800|         8250|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|         2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|         4450|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|         5950|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|         8450|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|         8450|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|         9600|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|         3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|         5975|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|         7075|\n",
      "+-----+------+---------+------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.sum('sal').over(windowSpec).alias('running_total')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----------+\n",
      "|empno| ename|      job|deptno| sal|moving_avg|\n",
      "+-----+------+---------+------+----+----------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    5000.0|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|    3725.0|\n",
      "| 7369| SMITH|    CLERK|    10| 800|    2750.0|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    2850.0|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    2225.0|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|   1983.33|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1800.0|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1690.0|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1600.0|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    3000.0|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    2987.5|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|   2358.33|\n",
      "+-----+------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.round(F.avg('sal').over(windowSpec), 2).alias('moving_avg')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Next Salary within each department:\n",
    "Lead function allows us to compare current row with subsequent rows within each partition depending on the second argument (offset) which is by default set to 1 i.e. next row but we can change that parameter 2 to compare against every other row. The 3rd parameter is default value to be returned when no subsequent values exists or null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using lead() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|next_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    2450|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     800|\n",
      "| 7369| SMITH|    CLERK|    10| 800|    null|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    1600|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1500|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1250|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1250|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1150|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    null|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    2975|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    1100|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    null|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "    \"\"\"SELECT empno, ename, job, deptno, sal, LEAD(sal, 1) OVER (PARTITION BY deptno ORDER BY sal DESC) AS next_val\n",
    "      |FROM emp\n",
    "    \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using lead() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|next_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    2450|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     800|\n",
      "| 7369| SMITH|    CLERK|    10| 800|       0|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    1600|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1500|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1250|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1250|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1150|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|       0|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    2975|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    1100|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|       0|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.lead('sal', count=1, default=0).over(windowSpec).alias('next_val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Previous Salary within each department:\n",
    "Lag function allows us to compare current row with preceding rows within each partition depending on the second argument (offset) which is by default set to 1 i.e. next row but we can change that parameter 2 to compare against every other row. The 3rd parameter is default value to be returned when no subsequent values exists or null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using lag() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|prev_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    null|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|    5000|\n",
      "| 7369| SMITH|    CLERK|    10| 800|    2450|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    null|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    2850|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1600|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1500|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1250|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1250|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    null|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    3000|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    2975|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "    \"\"\"SELECT empno, ename, job, deptno, sal, LAG(sal, 1) OVER (PARTITION BY deptno ORDER BY sal DESC) AS prev_val\n",
    "      |FROM emp\n",
    "    \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using lag() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|prev_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|       0|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|    5000|\n",
      "| 7369| SMITH|    CLERK|    10| 800|    2450|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|       0|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    2850|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1600|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1500|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1250|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1250|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|       0|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    3000|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    2975|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.lag('sal', count=1, default=0).over(windowSpec).alias('prev_val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 First Salary within each department:\n",
    "First value within each partition i.e. highest salary (we are using order by descending on the salary) within each department can be compared against every member within each department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using first() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+---------+\n",
      "|empno| ename|      job|deptno| sal|first_val|\n",
      "+-----+------+---------+------+----+---------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|     5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     5000|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     5000|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|     2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|     2850|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|     2850|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|     2850|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|     2850|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|     2850|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|     3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|     3000|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|     3000|\n",
      "+-----+------+---------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "\"\"\"SELECT empno, ename, job, deptno, sal, FIRST_VALUE(sal) OVER (PARTITION BY deptno ORDER BY sal DESC) AS first_val\n",
    "  |FROM emp\n",
    "\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using first() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+---------+\n",
      "|empno| ename|      job|deptno| sal|first_val|\n",
      "+-----+------+---------+------+----+---------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|     5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     5000|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     5000|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|     2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|     2850|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|     2850|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|     2850|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|     2850|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|     2850|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|     3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|     3000|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|     3000|\n",
      "+-----+------+---------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.first('sal').over(windowSpec).alias('first_val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Last Salary within each department:\n",
    "Last value within each partition i.e. lowest salary (we are using order by descending on the salary) within each department can be compared against every member within each department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using last() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|last_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|    2450|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     800|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1600|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1500|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1250|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1250|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1150|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    2975|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    1100|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, LAST_VALUE(sal) OVER (PARTITION BY deptno ORDER BY sal DESC) AS last_val\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops!** what happened here the `last_val` has the same value as in `sal` column but we were expecting the lowest salary within the department in the last_val column so for that we really need to understand how the window operates and works. There are two types of frames `ROW` and `RANGE`.The details are explained in this [posts](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html) from databricks.\n",
    "\n",
    "*This happens because **default window frame is range between unbounded preceding and current row**, so the `last_value()` never looks beyond current row unless we change the frame.*\n",
    "\n",
    "Last value fixed by supplying the window frame for `last_val()` to operate on. We will be using start frame current row and end frame unbounded following to get the last value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|last_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|     800|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     800|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     800|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    1150|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1150|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1150|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1150|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1150|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1150|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    1100|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    1100|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    1100|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, \n",
    "          |    LAST_VALUE(sal) OVER (PARTITION BY deptno ORDER BY sal DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS last_val\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using last() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc()).rowsBetween(Window.currentRow, Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|last_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|     800|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     800|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     800|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    1150|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1150|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1150|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1150|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1150|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1150|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    1100|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    1100|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    1100|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.last('sal').over(windowSpec).alias('last_val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Product Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_revenues = [\n",
    "  (\"Thin\",       \"cell phone\", 6000),\n",
    "  (\"Normal\",     \"tablet\",     1500),\n",
    "  (\"Mini\",       \"tablet\",     5500),\n",
    "  (\"Ultra thin\", \"cell phone\", 5000),\n",
    "  (\"Very thin\",  \"cell phone\", 6000),\n",
    "  (\"Big\",        \"tablet\",     2500),\n",
    "  (\"Bendable\",   \"cell phone\", 3000),\n",
    "  (\"Foldable\",   \"cell phone\", 3000),\n",
    "  (\"Pro\",        \"tablet\",     4500),\n",
    "  (\"Pro2\",       \"tablet\",     6500)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_rev_df = spark.createDataFrame(product_revenues, [\"product\", \"category\", \"revenue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|   product|  category|revenue|\n",
      "+----------+----------+-------+\n",
      "|      Thin|cell phone|   6000|\n",
      "|    Normal|    tablet|   1500|\n",
      "|      Mini|    tablet|   5500|\n",
      "|Ultra thin|cell phone|   5000|\n",
      "| Very thin|cell phone|   6000|\n",
      "|       Big|    tablet|   2500|\n",
      "|  Bendable|cell phone|   3000|\n",
      "|  Foldable|cell phone|   3000|\n",
      "|       Pro|    tablet|   4500|\n",
      "|      Pro2|    tablet|   6500|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_rev_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- revenue: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_rev_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_rev_df.createOrReplaceTempView(\"prod_rev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What are the best-selling and the second best-selling products in every category?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using dense_rank() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----------+\n",
      "|   product|  category|revenue|dense_rank|\n",
      "+----------+----------+-------+----------+\n",
      "|      Pro2|    tablet|   6500|         1|\n",
      "|      Mini|    tablet|   5500|         2|\n",
      "|      Thin|cell phone|   6000|         1|\n",
      "| Very thin|cell phone|   6000|         1|\n",
      "|Ultra thin|cell phone|   5000|         2|\n",
      "+----------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT product, category, revenue, dense_rank\n",
    "          |FROM (\n",
    "          |    SELECT product, category, revenue, \n",
    "          |    DENSE_RANK(revenue) OVER (PARTITION BY category ORDER BY revenue DESC) AS dense_rank\n",
    "          |    FROM prod_rev ) a\n",
    "          |WHERE dense_rank <= 2\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using dense_rank() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('category')).orderBy(col('revenue').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----------+\n",
      "|   product|  category|revenue|dense_rank|\n",
      "+----------+----------+-------+----------+\n",
      "|      Pro2|    tablet|   6500|         1|\n",
      "|      Mini|    tablet|   5500|         2|\n",
      "|      Thin|cell phone|   6000|         1|\n",
      "| Very thin|cell phone|   6000|         1|\n",
      "|Ultra thin|cell phone|   5000|         2|\n",
      "+----------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(prod_rev_df\n",
    " .select('product', 'category', 'revenue', F.dense_rank().over(windowSpec).alias('dense_rank'))\n",
    " .where('dense_rank <= 2')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 What is the difference between the revenue of each product and the revenue of the best selling product in the same category as that product?\n",
    "\n",
    "A point to note here is that since we are only interested in the maximm revenue of each product within a category we need to scan over the entire rows within that category. So using either rangeBetween and rowsBetween will work the same in this case.\n",
    "\n",
    "However, there is a difference between the `ROW` Frame and `RANGE` Frame:\n",
    "`ROW` frames are based on physical offsets from the position of the current input row, which means that `CURRENT ROW`, `[value] PRECEDING`, or `[value] FOLLOWING` specifies a physical offset. If `CURRENT ROW` is used as a boundary, it represents the current input row. `[value] PRECEDING` and `[value] FOLLOWING` describes the number of rows appear before and after the current input row, respectively.\n",
    "\n",
    "`RANGE` frames are based on logical offsets from the position of the current input row, and have similar syntax to the `ROW` frame. A logical offset is the difference between the value of the ordering expression of the current input row and the value of that same expression of the boundary row of the frame. Because of this definition, when a `RANGE` frame is used, only a single ordering expression is allowed. Also, for a RANGE frame, all rows having the same value of the ordering expression with the current input row are considered as same row as far as the boundary calculation is concerned.\n",
    "\n",
    "This will be clarified with Customer Expense Dataset below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using ROWS BETWEEN with Window Function:**\n",
    "\n",
    "To realise this use case we can use either `ROWS BETWEEN` or `RANGE BETWEEN`. Both will yield the same result because we are interested int the maximum revenue and this is not any calculation or aggregation over a certain number of wors or range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+\n",
      "|   product|  category|revenue|rev_diff|\n",
      "+----------+----------+-------+--------+\n",
      "|      Pro2|    tablet|   6500|       0|\n",
      "|      Mini|    tablet|   5500|    1000|\n",
      "|       Pro|    tablet|   4500|    2000|\n",
      "|       Big|    tablet|   2500|    4000|\n",
      "|    Normal|    tablet|   1500|    5000|\n",
      "|      Thin|cell phone|   6000|       0|\n",
      "| Very thin|cell phone|   6000|       0|\n",
      "|Ultra thin|cell phone|   5000|    1000|\n",
      "|  Bendable|cell phone|   3000|    3000|\n",
      "|  Foldable|cell phone|   3000|    3000|\n",
      "+----------+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using ROWS BETWEE\n",
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT product, category, revenue, \n",
    "          |    MAX(revenue) OVER (PARTITION BY category ORDER BY revenue DESC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) - revenue AS rev_diff \n",
    "          |FROM prod_rev\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+\n",
      "|   product|  category|revenue|rev_diff|\n",
      "+----------+----------+-------+--------+\n",
      "|      Pro2|    tablet|   6500|       0|\n",
      "|      Mini|    tablet|   5500|    1000|\n",
      "|       Pro|    tablet|   4500|    2000|\n",
      "|       Big|    tablet|   2500|    4000|\n",
      "|    Normal|    tablet|   1500|    5000|\n",
      "|      Thin|cell phone|   6000|       0|\n",
      "| Very thin|cell phone|   6000|       0|\n",
      "|Ultra thin|cell phone|   5000|    1000|\n",
      "|  Bendable|cell phone|   3000|    3000|\n",
      "|  Foldable|cell phone|   3000|    3000|\n",
      "+----------+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using RANGE BETWEEN\n",
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT product, category, revenue, \n",
    "          |    MAX(revenue) OVER (PARTITION BY category ORDER BY revenue DESC RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) - revenue AS rev_diff \n",
    "          |FROM prod_rev\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above section was to demonstrate where `ROWS BETWEEN` or `RANGE BETWEEN` would not make any difference and those examples served their purpose. But this use case can be better realized using `FIRST_VALUE` and we can totally do way with the `ROWS BETWEEN` or `RANGE BETWEEN` constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+\n",
      "|   product|  category|revenue|rev_diff|\n",
      "+----------+----------+-------+--------+\n",
      "|      Pro2|    tablet|   6500|       0|\n",
      "|      Mini|    tablet|   5500|    1000|\n",
      "|       Pro|    tablet|   4500|    2000|\n",
      "|       Big|    tablet|   2500|    4000|\n",
      "|    Normal|    tablet|   1500|    5000|\n",
      "|      Thin|cell phone|   6000|       0|\n",
      "| Very thin|cell phone|   6000|       0|\n",
      "|Ultra thin|cell phone|   5000|    1000|\n",
      "|  Bendable|cell phone|   3000|    3000|\n",
      "|  Foldable|cell phone|   3000|    3000|\n",
      "+----------+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just use the FIRST_VALUE function and that should do the same thing\n",
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT product, category, revenue, \n",
    "          |    FIRST_VALUE(revenue) OVER (PARTITION BY category ORDER BY revenue DESC) - revenue AS rev_diff \n",
    "          |FROM prod_rev\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using rangeBetween() with Window Function:**\n",
    "\n",
    "To realise this use case we can use either `rowsBetween` or `rangeBetween`. Both will yield the same result because we are interested int the maximum revenue and this is not any calculation or aggregation over a certain number of wors or range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using .rangeBetween(...)\n",
    "windowSpec = (Window\n",
    "              .partitionBy(prod_rev_df['category'])\n",
    "              .orderBy(prod_rev_df['revenue'].desc())\n",
    "              .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+\n",
      "|   product|  category|revenue|rev_diff|\n",
      "+----------+----------+-------+--------+\n",
      "|      Pro2|    tablet|   6500|       0|\n",
      "|      Mini|    tablet|   5500|    1000|\n",
      "|       Pro|    tablet|   4500|    2000|\n",
      "|       Big|    tablet|   2500|    4000|\n",
      "|    Normal|    tablet|   1500|    5000|\n",
      "|      Thin|cell phone|   6000|       0|\n",
      "| Very thin|cell phone|   6000|       0|\n",
      "|Ultra thin|cell phone|   5000|    1000|\n",
      "|  Bendable|cell phone|   3000|    3000|\n",
      "|  Foldable|cell phone|   3000|    3000|\n",
      "+----------+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(prod_rev_df\n",
    " .select('product', 'category', 'revenue', (F.max('revenue').over(windowSpec) - col('revenue')).alias('rev_diff'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using .rowsBetween(...) as well\n",
    "windowSpec = (Window\n",
    "              .partitionBy(prod_rev_df['category'])\n",
    "              .orderBy(prod_rev_df['revenue'].desc())\n",
    "              .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+\n",
      "|   product|  category|revenue|rev_diff|\n",
      "+----------+----------+-------+--------+\n",
      "|      Pro2|    tablet|   6500|       0|\n",
      "|      Mini|    tablet|   5500|    1000|\n",
      "|       Pro|    tablet|   4500|    2000|\n",
      "|       Big|    tablet|   2500|    4000|\n",
      "|    Normal|    tablet|   1500|    5000|\n",
      "|      Thin|cell phone|   6000|       0|\n",
      "| Very thin|cell phone|   6000|       0|\n",
      "|Ultra thin|cell phone|   5000|    1000|\n",
      "|  Bendable|cell phone|   3000|    3000|\n",
      "|  Foldable|cell phone|   3000|    3000|\n",
      "+----------+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(prod_rev_df\n",
    " .select('product', 'category', 'revenue', (F.max('revenue').over(windowSpec) - col('revenue')).alias('rev_diff'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customer Expense Data\n",
    "\n",
    "In this example dataset, there are three customers who have spent different amounts of money each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expense = [\n",
    "    ('Alice', '05/01/2016', 50),\n",
    "    ('Alice', '05/02/2016', 45),\n",
    "    ('Alice', '05/02/2016', 14),\n",
    "    ('Alice', '05/03/2016', 55),\n",
    "    ('Alice', '05/04/2016', 47),\n",
    "    ('Alice', '05/05/2016', 27),\n",
    "    ('Alice', '05/05/2016', 19),\n",
    "    ('Bob', '05/01/2016', 25),\n",
    "    ('Bob', '05/01/2016', 56),\n",
    "    ('Bob', '05/04/2016', 29),\n",
    "    ('Bob', '05/09/2016', 27),\n",
    "    ('Bob', '05/10/2016', 34),\n",
    "    ('Bob', '05/11/2016', 67),\n",
    "    ('Bob', '05/11/2016', 8),\n",
    "    ('Bob', '05/12/2016', 32),\n",
    "    ('Bob', '05/13/2016', 10),\n",
    "    ('Charlie', '05/04/2016', 27),\n",
    "    ('Charlie', '05/11/2016', 67)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expense_df = spark.createDataFrame(expense, [\"name\", \"order_date\", \"amount_spent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert order_date into date format\n",
    "expense_df = expense_df.withColumn('order_date', F.to_date(col(\"order_date\"), 'MM/dd/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+\n",
      "|   name|order_date|amount_spent|\n",
      "+-------+----------+------------+\n",
      "|  Alice|2016-05-01|          50|\n",
      "|  Alice|2016-05-02|          45|\n",
      "|  Alice|2016-05-02|          14|\n",
      "|  Alice|2016-05-03|          55|\n",
      "|  Alice|2016-05-04|          47|\n",
      "|  Alice|2016-05-05|          27|\n",
      "|  Alice|2016-05-05|          19|\n",
      "|    Bob|2016-05-01|          25|\n",
      "|    Bob|2016-05-01|          56|\n",
      "|    Bob|2016-05-04|          29|\n",
      "|    Bob|2016-05-09|          27|\n",
      "|    Bob|2016-05-10|          34|\n",
      "|    Bob|2016-05-11|          67|\n",
      "|    Bob|2016-05-11|           8|\n",
      "|    Bob|2016-05-12|          32|\n",
      "|    Bob|2016-05-13|          10|\n",
      "|Charlie|2016-05-04|          27|\n",
      "|Charlie|2016-05-11|          67|\n",
      "+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expense_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expense_df.createOrReplaceTempView(\"expense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- amount_spent: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expense_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What is the first Purchase Date for each customer:\n",
    "We are interested in finding when Alice, Bob and Charlie made there first ever purchase? We can extend that to answer the question is the given purchase customer's first purchase or the customer has returned to purchase more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using FIRST with Window Function:**\n",
    "\n",
    "We use `FIRST` function in conjunction with `WINDOW` analytics to find the first purchase date for each customer. If the `first_order_date` is same as the `order_date` in the current row then this row is the customer's first transaction otherwise its a returning cutomer transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+------------------+\n",
      "|   name|order_date|first_order_date|returning_customer|\n",
      "+-------+----------+----------------+------------------+\n",
      "|  Alice|2016-05-01|      2016-05-01|                 0|\n",
      "|  Alice|2016-05-02|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-02|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-03|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-04|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-05|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-05|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-01|      2016-05-01|                 0|\n",
      "|    Bob|2016-05-01|      2016-05-01|                 0|\n",
      "|    Bob|2016-05-04|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-09|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-10|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-11|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-11|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-12|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-13|      2016-05-01|                 1|\n",
      "|Charlie|2016-05-04|      2016-05-04|                 0|\n",
      "|Charlie|2016-05-11|      2016-05-04|                 1|\n",
      "+-------+----------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT name, \n",
    "          |       order_date, \n",
    "          |       FIRST(order_date) OVER (PARTITION BY name ORDER BY order_date) AS first_order_date, \n",
    "          |       CASE \n",
    "          |           WHEN FIRST(order_date) OVER (PARTITION BY name ORDER BY order_date) = order_date then 0 else 1 \n",
    "          |       END AS returning_customer\n",
    "          |FROM expense\n",
    "          |ORDER BY name, order_date\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then filter on the `returning_customer` column to filter out only the first purchase date. We do `distinct` here because the customer can make more than one purchase on the first purchase date as we can see in the case of Bob on 2016-05-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|order_date|\n",
      "+-------+----------+\n",
      "|  Alice|2016-05-01|\n",
      "|    Bob|2016-05-01|\n",
      "|Charlie|2016-05-04|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT DISTINCT name, order_date \n",
    "          |FROM(\n",
    "          |    SELECT name, \n",
    "          |       order_date, \n",
    "          |       FIRST(order_date) OVER (PARTITION BY name ORDER BY order_date) AS first_order_date, \n",
    "          |       CASE \n",
    "          |           WHEN FIRST(order_date) OVER (PARTITION BY name ORDER BY order_date) = order_date then 0 else 1  \n",
    "          |       END AS returning_customer\n",
    "          |    FROM expense\n",
    "          |    ) \n",
    "          |WHERE returning_customer = 0\n",
    "          |ORDER BY name, order_date\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using FIRST with Window Function:**\n",
    "\n",
    "We can achieve the same thing using DataFrame APIs as well.\n",
    "\n",
    "We use `FIRST` function in conjunction with `WINDOW` analytics to find the first purchase date for each customer. If the `first_order_date` is same as the `order_date` in the current row then this row is the customer's first transaction otherwise its a returning cutomer transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = (Window\n",
    "              .partitionBy(expense_df['name'])\n",
    "              .orderBy(expense_df['order_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+------------------+\n",
      "|   name|order_date|first_order_date|returning_customer|\n",
      "+-------+----------+----------------+------------------+\n",
      "|  Alice|2016-05-01|      2016-05-01|                 0|\n",
      "|  Alice|2016-05-02|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-02|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-03|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-04|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-05|      2016-05-01|                 1|\n",
      "|  Alice|2016-05-05|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-01|      2016-05-01|                 0|\n",
      "|    Bob|2016-05-01|      2016-05-01|                 0|\n",
      "|    Bob|2016-05-04|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-09|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-10|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-11|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-11|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-12|      2016-05-01|                 1|\n",
      "|    Bob|2016-05-13|      2016-05-01|                 1|\n",
      "|Charlie|2016-05-04|      2016-05-04|                 0|\n",
      "|Charlie|2016-05-11|      2016-05-04|                 1|\n",
      "+-------+----------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(expense_df\n",
    " .select('name', \n",
    "         'order_date', \n",
    "         F.first('order_date').over(windowSpec).alias('first_order_date'), \n",
    "         F.when(F.first('order_date').over(windowSpec) == col('order_date'), 0).otherwise(1).alias('returning_customer'))\n",
    " .orderBy('name', 'order_date')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then filter on the `returning_customer` column to filter out only the first purchase date. We do `distinct` here because the customer can make more than one purchase on the first purchase date as is seen here for the cutomer Bob on 2016-05-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|order_date|\n",
      "+-------+----------+\n",
      "|  Alice|2016-05-01|\n",
      "|    Bob|2016-05-01|\n",
      "|Charlie|2016-05-04|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(expense_df\n",
    " .select('name', \n",
    "         'order_date', \n",
    "         F.first('order_date').over(windowSpec).alias('first_order_date'), \n",
    "         F.when(F.first('order_date').over(windowSpec) == col('order_date'), 0).otherwise(1).alias('returning_customer'))\n",
    " .filter(col(\"returning_customer\") == 0)\n",
    " .distinct()\n",
    " .select('name', 'order_date')\n",
    " .orderBy('name', 'order_date')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 What is the cumulative amount spent by each customer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using ROWS BETWEEN with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+-----------+\n",
      "|   name|order_date|amount_spent|cum_expense|\n",
      "+-------+----------+------------+-----------+\n",
      "|  Alice|2016-05-01|          50|         50|\n",
      "|  Alice|2016-05-02|          45|         95|\n",
      "|  Alice|2016-05-02|          14|        109|\n",
      "|  Alice|2016-05-03|          55|        164|\n",
      "|  Alice|2016-05-04|          47|        211|\n",
      "|  Alice|2016-05-05|          27|        238|\n",
      "|  Alice|2016-05-05|          19|        257|\n",
      "|    Bob|2016-05-01|          25|         25|\n",
      "|    Bob|2016-05-01|          56|         81|\n",
      "|    Bob|2016-05-04|          29|        110|\n",
      "|    Bob|2016-05-09|          27|        137|\n",
      "|    Bob|2016-05-10|          34|        171|\n",
      "|    Bob|2016-05-11|           8|        246|\n",
      "|    Bob|2016-05-11|          67|        238|\n",
      "|    Bob|2016-05-12|          32|        278|\n",
      "|    Bob|2016-05-13|          10|        288|\n",
      "|Charlie|2016-05-04|          27|         27|\n",
      "|Charlie|2016-05-11|          67|         94|\n",
      "+-------+----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "\"\"\"SELECT name, order_date, amount_spent, \n",
    "  |    ROUND(SUM(amount_spent) OVER (PARTITION BY name ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW), 2) AS cum_expense\n",
    "  |FROM expense\n",
    "  |ORDER BY name, order_date\n",
    "\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using ROWS BETWEEN with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = (Window\n",
    "              .partitionBy(expense_df['name'])\n",
    "              .orderBy(expense_df['order_date'])\n",
    "              .rowsBetween(Window.unboundedPreceding, Window.currentRow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+-----------+\n",
      "|   name|order_date|amount_spent|cum_expense|\n",
      "+-------+----------+------------+-----------+\n",
      "|  Alice|2016-05-01|          50|         50|\n",
      "|  Alice|2016-05-02|          45|         95|\n",
      "|  Alice|2016-05-02|          14|        109|\n",
      "|  Alice|2016-05-03|          55|        164|\n",
      "|  Alice|2016-05-04|          47|        211|\n",
      "|  Alice|2016-05-05|          27|        238|\n",
      "|  Alice|2016-05-05|          19|        257|\n",
      "|    Bob|2016-05-01|          25|         25|\n",
      "|    Bob|2016-05-01|          56|         81|\n",
      "|    Bob|2016-05-04|          29|        110|\n",
      "|    Bob|2016-05-09|          27|        137|\n",
      "|    Bob|2016-05-10|          34|        171|\n",
      "|    Bob|2016-05-11|           8|        246|\n",
      "|    Bob|2016-05-11|          67|        238|\n",
      "|    Bob|2016-05-12|          32|        278|\n",
      "|    Bob|2016-05-13|          10|        288|\n",
      "|Charlie|2016-05-04|          27|         27|\n",
      "|Charlie|2016-05-11|          67|         94|\n",
      "+-------+----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(expense_df\n",
    " .select('name', 'order_date', 'amount_spent', F.round(F.sum('amount_spent').over(windowSpec), 2).alias('cum_expense'))\n",
    " .orderBy('name', 'order_date')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 What is the moving average between 3 consecutive transactions [previous, current, next] for each customer:\n",
    "\n",
    "Note the difference between this and the next use case where we will see that the **moving average of last three transactions is not as same as moving average of consecutive three days' transaction**. So, we have to use ROWS BETWEEN and we cannot use RANGE BETWEEN because we have to stick to the bounds between previous and next row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using ROWS BETWEEN with specific values with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+\n",
      "|   name|order_date|amount_spent|moving_avg|\n",
      "+-------+----------+------------+----------+\n",
      "|  Alice|2016-05-01|          50|      47.5|\n",
      "|  Alice|2016-05-02|          45|     36.33|\n",
      "|  Alice|2016-05-02|          14|      38.0|\n",
      "|  Alice|2016-05-03|          55|     38.67|\n",
      "|  Alice|2016-05-04|          47|      43.0|\n",
      "|  Alice|2016-05-05|          19|      23.0|\n",
      "|  Alice|2016-05-05|          27|      31.0|\n",
      "|    Bob|2016-05-01|          56|     36.67|\n",
      "|    Bob|2016-05-01|          25|      40.5|\n",
      "|    Bob|2016-05-04|          29|     37.33|\n",
      "|    Bob|2016-05-09|          27|      30.0|\n",
      "|    Bob|2016-05-10|          34|     42.67|\n",
      "|    Bob|2016-05-11|           8|     35.67|\n",
      "|    Bob|2016-05-11|          67|     36.33|\n",
      "|    Bob|2016-05-12|          32|     16.67|\n",
      "|    Bob|2016-05-13|          10|      21.0|\n",
      "|Charlie|2016-05-04|          27|      47.0|\n",
      "|Charlie|2016-05-11|          67|      47.0|\n",
      "+-------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "\"\"\"SELECT name, order_date, amount_spent, \n",
    "  |    ROUND(AVG(amount_spent) OVER (PARTITION BY name ORDER BY order_date ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), 2) AS moving_avg\n",
    "  |FROM expense\n",
    "  |ORDER BY name, order_date\n",
    "\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using ROWS BETWEEN with specific values with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = (Window\n",
    "              .partitionBy(expense_df['name'])\n",
    "              .orderBy(expense_df['order_date'])\n",
    "              .rowsBetween(-1, 1)) # note the usage of specific rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+\n",
      "|   name|order_date|amount_spent|moving_avg|\n",
      "+-------+----------+------------+----------+\n",
      "|  Alice|2016-05-01|          50|      47.5|\n",
      "|  Alice|2016-05-02|          45|     36.33|\n",
      "|  Alice|2016-05-02|          14|      38.0|\n",
      "|  Alice|2016-05-03|          55|     38.67|\n",
      "|  Alice|2016-05-04|          47|      43.0|\n",
      "|  Alice|2016-05-05|          27|      31.0|\n",
      "|  Alice|2016-05-05|          19|      23.0|\n",
      "|    Bob|2016-05-01|          25|      40.5|\n",
      "|    Bob|2016-05-01|          56|     36.67|\n",
      "|    Bob|2016-05-04|          29|     37.33|\n",
      "|    Bob|2016-05-09|          27|      30.0|\n",
      "|    Bob|2016-05-10|          34|     42.67|\n",
      "|    Bob|2016-05-11|           8|     35.67|\n",
      "|    Bob|2016-05-11|          67|     36.33|\n",
      "|    Bob|2016-05-12|          32|     16.67|\n",
      "|    Bob|2016-05-13|          10|      21.0|\n",
      "|Charlie|2016-05-04|          27|      47.0|\n",
      "|Charlie|2016-05-11|          67|      47.0|\n",
      "+-------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(expense_df\n",
    " .select('name', 'order_date', 'amount_spent', F.round(F.avg('amount_spent').over(windowSpec), 2).alias('moving_avg'))\n",
    " .orderBy('name', 'order_date')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What is the 3 days moving average for each customer:\n",
    "\n",
    "The difference between this and the previous use case is that **there can be 1, 2, or N i.e. any number of transactions within past three days and not necessary only 3 transactions in last 3 days**. So, we have to use RANGE BETWEEN and we cannot use ROWS BETWEEN because we do not know how many rows wil be there in last 3 days before the current day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|order_date|order_timestamp|\n",
      "+----------+---------------+\n",
      "|2016-05-01|     1462060800|\n",
      "|2016-05-02|     1462147200|\n",
      "|2016-05-02|     1462147200|\n",
      "|2016-05-03|     1462233600|\n",
      "|2016-05-04|     1462320000|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert order_date into long using typcasting\n",
    "expense_df.select('order_date', col('order_date').cast('timestamp').cast('long').alias('order_timestamp')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|order_date|order_timestamp|\n",
      "+----------+---------------+\n",
      "|2016-05-01|     1462060800|\n",
      "|2016-05-02|     1462147200|\n",
      "|2016-05-02|     1462147200|\n",
      "|2016-05-03|     1462233600|\n",
      "|2016-05-04|     1462320000|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert order_date into long using inbuilt pyspark sql function unix_timestamp\n",
    "expense_df.select('order_date', F.unix_timestamp(col('order_date')).alias('order_timestamp')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above trick to convert the order_date column into long and then compare with the 3 days previous timestamp long value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using RANGE BETWEEN with specific values with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a lambda function to return the prev_days * timestamp in long\n",
    "days = lambda d: d * 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we have to use\n",
    "windowSpec = (Window\n",
    "              .partitionBy(expense_df['name'])\n",
    "              .orderBy(F.unix_timestamp(expense_df['order_date']))\n",
    "              .rangeBetween(-days(2), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+------------+----------+\n",
      "|   name|order_date|prev_3rd_day|amount_spent|moving_avg|\n",
      "+-------+----------+------------+------------+----------+\n",
      "|  Alice|2016-05-01|  2016-04-29|          50|      50.0|\n",
      "|  Alice|2016-05-02|  2016-04-30|          45|     36.33|\n",
      "|  Alice|2016-05-02|  2016-04-30|          14|     36.33|\n",
      "|  Alice|2016-05-03|  2016-05-01|          55|      41.0|\n",
      "|  Alice|2016-05-04|  2016-05-02|          47|     40.25|\n",
      "|  Alice|2016-05-05|  2016-05-03|          27|      37.0|\n",
      "|  Alice|2016-05-05|  2016-05-03|          19|      37.0|\n",
      "|    Bob|2016-05-01|  2016-04-29|          25|      40.5|\n",
      "|    Bob|2016-05-01|  2016-04-29|          56|      40.5|\n",
      "|    Bob|2016-05-04|  2016-05-02|          29|      29.0|\n",
      "|    Bob|2016-05-09|  2016-05-07|          27|      27.0|\n",
      "|    Bob|2016-05-10|  2016-05-08|          34|      30.5|\n",
      "|    Bob|2016-05-11|  2016-05-09|           8|      34.0|\n",
      "|    Bob|2016-05-11|  2016-05-09|          67|      34.0|\n",
      "|    Bob|2016-05-12|  2016-05-10|          32|     35.25|\n",
      "|    Bob|2016-05-13|  2016-05-11|          10|     29.25|\n",
      "|Charlie|2016-05-04|  2016-05-02|          27|      27.0|\n",
      "|Charlie|2016-05-11|  2016-05-09|          67|      67.0|\n",
      "+-------+----------+------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(expense_df\n",
    " .select('name', 'order_date', F.date_sub('order_date', 2).alias('prev_3rd_day'), 'amount_spent', \n",
    "         F.round(F.avg('amount_spent').over(windowSpec), 2).alias('moving_avg'))\n",
    " .orderBy('name', 'order_date')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe closely the moving_avg for 2nd and 3rd rows? So, while it was calculating for 2nd row it should have taken only amounts 50 and 45 and the average should have been 47.5 but **OOPS!** the average is 36.33.\n",
    "\n",
    "Why is that so? The definition of Range Frames states that **Also, for a RANGE frame, all rows having the same value of the ordering expression with the current input row are considered as same row as far as the boundary calculation is concerned.**\n",
    "\n",
    "So, while it was calculating for 2nd row it also took the 3rd row because the ordering expression is over `order_date` and the 2nd and 3rd rows have same `order_date` and they are considered same. Although, we restricted the window range between current row and previous three dates it still took the next row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL API: Using RANGE BETWEEN with specific values with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172800"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 days in millis\n",
    "2 * 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+------------+----------+\n",
      "|   name|order_date|prev_3rd_day|amount_spent|moving_avg|\n",
      "+-------+----------+------------+------------+----------+\n",
      "|  Alice|2016-05-01|  2016-04-28|          50|      50.0|\n",
      "|  Alice|2016-05-02|  2016-04-29|          45|     36.33|\n",
      "|  Alice|2016-05-02|  2016-04-29|          14|     36.33|\n",
      "|  Alice|2016-05-03|  2016-04-30|          55|      41.0|\n",
      "|  Alice|2016-05-04|  2016-05-01|          47|     40.25|\n",
      "|  Alice|2016-05-05|  2016-05-02|          27|      37.0|\n",
      "|  Alice|2016-05-05|  2016-05-02|          19|      37.0|\n",
      "|    Bob|2016-05-01|  2016-04-28|          25|      40.5|\n",
      "|    Bob|2016-05-01|  2016-04-28|          56|      40.5|\n",
      "|    Bob|2016-05-04|  2016-05-01|          29|      29.0|\n",
      "|    Bob|2016-05-09|  2016-05-06|          27|      27.0|\n",
      "|    Bob|2016-05-10|  2016-05-07|          34|      30.5|\n",
      "|    Bob|2016-05-11|  2016-05-08|           8|      34.0|\n",
      "|    Bob|2016-05-11|  2016-05-08|          67|      34.0|\n",
      "|    Bob|2016-05-12|  2016-05-09|          32|     35.25|\n",
      "|    Bob|2016-05-13|  2016-05-10|          10|     29.25|\n",
      "|Charlie|2016-05-04|  2016-05-01|          27|      27.0|\n",
      "|Charlie|2016-05-11|  2016-05-08|          67|      67.0|\n",
      "+-------+----------+------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "\"\"\"SELECT name, order_date, DATE_SUB(order_date, 3) AS prev_3rd_day, amount_spent, \n",
    "  |    ROUND(AVG(amount_spent) OVER (PARTITION BY name ORDER BY UNIX_TIMESTAMP(order_date) RANGE BETWEEN 172800 PRECEDING AND CURRENT ROW), 2) AS moving_avg\n",
    "  |FROM expense\n",
    "  |ORDER BY name, order_date\n",
    "\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tractor Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = [\n",
    "    (1, 'Jan-03', 141),\n",
    "    (2, 'Feb-03', 157),\n",
    "    (3, 'Mar-03', 185),\n",
    "    (4, 'Apr-03', 199),\n",
    "    (5, 'May-03', 203),\n",
    "    (6, 'Jun-03', 189),\n",
    "    (7, 'Jul-03', 207),\n",
    "    (8, 'Aug-03', 207),\n",
    "    (9, 'Sep-03', 171),\n",
    "    (10, 'Oct-03', 150),\n",
    "    (11, 'Nov-03', 138),\n",
    "    (12, 'Dec-03', 165),\n",
    "    (13, 'Jan-04', 145),\n",
    "    (14, 'Feb-04', 168),\n",
    "    (15, 'Mar-04', 197),\n",
    "    (16, 'Apr-04', 208),\n",
    "    (17, 'May-04', 210),\n",
    "    (18, 'Jun-04', 209),\n",
    "    (19, 'Jul-04', 238),\n",
    "    (20, 'Aug-04', 238)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(sales, ['tid', 'month_year', 'sales']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tid: long (nullable = true)\n",
      " |-- month_year: string (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Running Difference of Current Sale from the Previous Sale:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using lag() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "|month_year|sales|sales_lag_1|sales_lag_2|sales_lag_3|diff_with_last_sales|\n",
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "|    Jan-03|  141|       null|       null|       null|                null|\n",
      "|    Feb-03|  157|        141|       null|       null|                  16|\n",
      "|    Mar-03|  185|        157|        141|       null|                  28|\n",
      "|    Apr-03|  199|        185|        157|        141|                  14|\n",
      "|    May-03|  203|        199|        185|        157|                   4|\n",
      "|    Jun-03|  189|        203|        199|        185|                 -14|\n",
      "|    Jul-03|  207|        189|        203|        199|                  18|\n",
      "|    Aug-03|  207|        207|        189|        203|                   0|\n",
      "|    Sep-03|  171|        207|        207|        189|                 -36|\n",
      "|    Oct-03|  150|        171|        207|        207|                 -21|\n",
      "|    Nov-03|  138|        150|        171|        207|                 -12|\n",
      "|    Dec-03|  165|        138|        150|        171|                  27|\n",
      "|    Jan-04|  145|        165|        138|        150|                 -20|\n",
      "|    Feb-04|  168|        145|        165|        138|                  23|\n",
      "|    Mar-04|  197|        168|        145|        165|                  29|\n",
      "|    Apr-04|  208|        197|        168|        145|                  11|\n",
      "|    May-04|  210|        208|        197|        168|                   2|\n",
      "|    Jun-04|  209|        210|        208|        197|                  -1|\n",
      "|    Jul-04|  238|        209|        210|        208|                  29|\n",
      "|    Aug-04|  238|        238|        209|        210|                   0|\n",
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT month_year, sales, \n",
    "          |    LAG(sales, 1) OVER (ORDER BY tid) AS sales_lag_1,\n",
    "          |    LAG(sales, 2) OVER (ORDER BY tid) AS sales_lag_2,\n",
    "          |    LAG(sales, 3) OVER (ORDER BY tid) AS sales_lag_3,\n",
    "          |    sales - LAG(sales, 1) OVER (ORDER BY tid) AS diff_with_last_sales\n",
    "          |FROM sales \n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using lag() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.orderBy(sales_df['tid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "|month_year|sales|sales_lag_1|sales_lag_2|sales_lag_3|diff_with_last_sales|\n",
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "|    Jan-03|  141|       null|       null|       null|                null|\n",
      "|    Feb-03|  157|        141|       null|       null|                  16|\n",
      "|    Mar-03|  185|        157|        141|       null|                  28|\n",
      "|    Apr-03|  199|        185|        157|        141|                  14|\n",
      "|    May-03|  203|        199|        185|        157|                   4|\n",
      "|    Jun-03|  189|        203|        199|        185|                 -14|\n",
      "|    Jul-03|  207|        189|        203|        199|                  18|\n",
      "|    Aug-03|  207|        207|        189|        203|                   0|\n",
      "|    Sep-03|  171|        207|        207|        189|                 -36|\n",
      "|    Oct-03|  150|        171|        207|        207|                 -21|\n",
      "|    Nov-03|  138|        150|        171|        207|                 -12|\n",
      "|    Dec-03|  165|        138|        150|        171|                  27|\n",
      "|    Jan-04|  145|        165|        138|        150|                 -20|\n",
      "|    Feb-04|  168|        145|        165|        138|                  23|\n",
      "|    Mar-04|  197|        168|        145|        165|                  29|\n",
      "|    Apr-04|  208|        197|        168|        145|                  11|\n",
      "|    May-04|  210|        208|        197|        168|                   2|\n",
      "|    Jun-04|  209|        210|        208|        197|                  -1|\n",
      "|    Jul-04|  238|        209|        210|        208|                  29|\n",
      "|    Aug-04|  238|        238|        209|        210|                   0|\n",
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(sales_df\n",
    " .select('month_year', 'sales', \n",
    "         F.lag('sales', count=1).over(windowSpec).alias('sales_lag_1'),\n",
    "         F.lag('sales', count=2).over(windowSpec).alias('sales_lag_2'),\n",
    "         F.lag('sales', count=3).over(windowSpec).alias('sales_lag_3'),\n",
    "         (col('sales') - F.lag('sales', count=1).over(windowSpec)).alias('diff_with_last_sales'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Forward-fill and Backward-fill missing data in Spark:\n",
    "\n",
    "This section is inpired from [https://johnpaton.net/posts/forward-fill-spark/](https://johnpaton.net/posts/forward-fill-spark/)\n",
    "\n",
    "Like the author, I faced the same problem while I started working with Apache Spark. I found that there is no forward fill and backward fill method in the Spark DataFrame just as in Pandas DataFrames. Such is the price of scalability. Since I was just starting with Spark so I applied groupBy on my Spark Dataframe then converted the grouped values of Spark Dataframe into pandas and then used the Pandas `.fillna` method, specifying specifying `method='ffill'` or `method='bfill'`. But once I stumbled on this blog this fell very well with the theme of this notebook.\n",
    "\n",
    "**A word of caution:** The following approaches will collect all rows of each group to some executor node. This will result in failed jobs if the number of rows in some groups is larger than the memory of our executor nodes.\n",
    "\n",
    "Imagine we are measuring the temperature in two spots in your back yard, one in the shade and one in the sun. We record a measurement every half hour so you can compare them. However, we got the cheapest possible digital thermometer, so a lot of the measurements end up missing. Our data may look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = [    \n",
    "    ('2017-09-09 12:00:00', 'shade', 18.830184076113213),\n",
    "    ('2017-09-09 12:01:00', 'sun',   None),\n",
    "    ('2017-09-09 12:30:00', 'shade', None),\n",
    "    ('2017-09-09 12:31:00', 'sun',   21.55237663805009),\n",
    "    ('2017-09-09 13:00:00', 'shade', 18.59059750682235),\n",
    "    ('2017-09-09 13:01:00', 'sun',   None),\n",
    "    ('2017-09-09 13:30:00', 'shade', None),\n",
    "    ('2017-09-09 13:31:00', 'sun',   22.587784977960474),\n",
    "    ('2017-09-09 14:00:00', 'shade', 19.101003724324197),\n",
    "    ('2017-09-09 14:01:00', 'sun',   20.548896316341516)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(weather, ['time', 'location', 'temp']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------------+\n",
      "|               time|location|              temp|\n",
      "+-------------------+--------+------------------+\n",
      "|2017-09-09 12:00:00|   shade|18.830184076113213|\n",
      "|2017-09-09 12:01:00|     sun|              null|\n",
      "|2017-09-09 12:30:00|   shade|              null|\n",
      "|2017-09-09 12:31:00|     sun| 21.55237663805009|\n",
      "|2017-09-09 13:00:00|   shade| 18.59059750682235|\n",
      "|2017-09-09 13:01:00|     sun|              null|\n",
      "|2017-09-09 13:30:00|   shade|              null|\n",
      "|2017-09-09 13:31:00|     sun|22.587784977960474|\n",
      "|2017-09-09 14:00:00|   shade|19.101003724324197|\n",
      "|2017-09-09 14:01:00|     sun|20.548896316341516|\n",
      "+-------------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_df.createOrReplaceTempView(\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the measurements each half hour (or maybe to do some machine learning), we need a way of filling in the missing measurements. If the value we are measuring (in this case temperature) changes slowly with respect to how frequently we make a measurement, then a forward fill may be a reasonable choice.\n",
    "\n",
    "In Pandas, this is easy. We just do a groupby without aggregation, and to each group apply the `.fillna` method, specifying specifying `method='ffill'`:\n",
    "\n",
    "`\n",
    "df_filled = df.groupby('location').apply(lambda group: group.fillna(method='ffill'))\n",
    "`\n",
    "\n",
    "We can take help of the `pyspark.sql window` function `last` and `first`. As its name suggests, `last` returns the last value in the window (implying that the window must have a meaningful ordering). It takes an optional argument `ignorenulls` which, when set to True, causes `last` to return the last non-null value in the window, if such a value exists. The same logic applies for `first` except it returns the first non-null value in the window.\n",
    "\n",
    "The strategy to forward fill in Spark is as follows. First we define a window, which is ordered in time, and which includes all the rows from the beginning of time up until the current row. We achieve this here simply by selecting the rows in the window as being the `rowsBetween Window.unboundedPreceding` (the largest negative value possible), and `Window.currentRow` (the current row). Specifying too large of a value for the rows doesn't cause any errors, so we can just use a very large number to be sure our window reaches until the very beginning of the dataframe. If we need to optimize memory usage, we can make our job much more efficient by finding the maximal number of consecutive nulls in our dataframe and only taking a large enough window to include all of those plus one non-null value. We partition the window by the `location` column to make sure that gaps only get filled with previous measurements from the same location.\n",
    "\n",
    "We act with `last` over the window we have defined, specifying `ignorenulls=True`. If the current row is non-null, then the output will just be the value of current row. However, if the current row is null, then the function will return the most recent (last) non-null value in the window.\n",
    "\n",
    "Similarly the strategy to backward fill in Spark is as follows. First we define a window, which is ordered in time, and which includes all the rows from the current row to up until the end of time. We achieve this here simply by selecting the rows in the window as being the `rowsBetween Window.currentRow` (the current row) and `Window.unboundedFollowing` (the largest positive value possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using last(), first() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the window\n",
    "fwd_window = (Window.partitionBy('location')\n",
    "               .orderBy('time')\n",
    "               .rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    "\n",
    "bckwd_window = (Window.partitionBy('location')\n",
    "               .orderBy('time')\n",
    "               .rowsBetween(Window.currentRow, Window.unboundedFollowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the forward-filled column\n",
    "ffilled_column = F.last(weather_df['temp'], ignorenulls=True).over(fwd_window) # True: fill with last non-null\n",
    "bfilled_column = F.first(weather_df['temp'], ignorenulls=True).over(bckwd_window) # True: fill with first non-null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the fill\n",
    "weather_df_filled = weather_df.withColumn('temp_fwd_filled', ffilled_column).withColumn('temp_bckwd_filled', bfilled_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|               time|location|              temp|   temp_fwd_filled| temp_bckwd_filled|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|2017-09-09 12:00:00|   shade|18.830184076113213|18.830184076113213|18.830184076113213|\n",
      "|2017-09-09 12:30:00|   shade|              null|18.830184076113213| 18.59059750682235|\n",
      "|2017-09-09 13:00:00|   shade| 18.59059750682235| 18.59059750682235| 18.59059750682235|\n",
      "|2017-09-09 13:30:00|   shade|              null| 18.59059750682235|19.101003724324197|\n",
      "|2017-09-09 14:00:00|   shade|19.101003724324197|19.101003724324197|19.101003724324197|\n",
      "|2017-09-09 12:01:00|     sun|              null|              null| 21.55237663805009|\n",
      "|2017-09-09 12:31:00|     sun| 21.55237663805009| 21.55237663805009| 21.55237663805009|\n",
      "|2017-09-09 13:01:00|     sun|              null| 21.55237663805009|22.587784977960474|\n",
      "|2017-09-09 13:31:00|     sun|22.587784977960474|22.587784977960474|22.587784977960474|\n",
      "|2017-09-09 14:01:00|     sun|20.548896316341516|20.548896316341516|20.548896316341516|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "weather_df_filled.orderBy('location', 'time').show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see all the null values have been forward filled with the last known non-null values and likewise for backward filling. Note that one row for forward fill is still null after forward filling because that was the first row in that group and it was null itself. It had no row to look back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using LAST_VALUE(), FIRST_VALUE() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|               time|location|              temp|   temp_fwd_filled|  temp_bckd_filled|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|2017-09-09 12:00:00|   shade|18.830184076113213|18.830184076113213|18.830184076113213|\n",
      "|2017-09-09 12:30:00|   shade|              null|18.830184076113213| 18.59059750682235|\n",
      "|2017-09-09 13:00:00|   shade| 18.59059750682235| 18.59059750682235| 18.59059750682235|\n",
      "|2017-09-09 13:30:00|   shade|              null| 18.59059750682235|19.101003724324197|\n",
      "|2017-09-09 14:00:00|   shade|19.101003724324197|19.101003724324197|19.101003724324197|\n",
      "|2017-09-09 12:01:00|     sun|              null|              null| 21.55237663805009|\n",
      "|2017-09-09 12:31:00|     sun| 21.55237663805009| 21.55237663805009| 21.55237663805009|\n",
      "|2017-09-09 13:01:00|     sun|              null| 21.55237663805009|22.587784977960474|\n",
      "|2017-09-09 13:31:00|     sun|22.587784977960474|22.587784977960474|22.587784977960474|\n",
      "|2017-09-09 14:01:00|     sun|20.548896316341516|20.548896316341516|20.548896316341516|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT time, location, temp, \n",
    "          |    LAST_VALUE(temp, TRUE) OVER (PARTITION BY location ORDER BY time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS temp_fwd_filled,\n",
    "          |    FIRST_VALUE(temp, TRUE) OVER (PARTITION BY location ORDER BY time ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS temp_bckd_filled\n",
    "          |FROM weather\n",
    "          |ORDER BY location, time\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Forward-fill and Backward-fill missing data without grouping:\n",
    "\n",
    "If the data is a univariate time-series and we need to fill missing values by forward or backward filling then we do not need to do any grouping or partitioning and just use the `orderBy` in the `Window` fucntion. However, this would bring all the data into into one partition in an executor. If the data set is large than can be hold in executor memory it wil result in OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using last(), first() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the window\n",
    "fwd_window = Window.orderBy('time').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "bckwd_window = Window.orderBy('time').rowsBetween(Window.currentRow, Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the forward-filled column\n",
    "ffilled_column = F.last(weather_df['temp'], ignorenulls=True).over(fwd_window) # True: fill with last non-null\n",
    "bfilled_column = F.first(weather_df['temp'], ignorenulls=True).over(bckwd_window) # True: fill with first non-null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the fill\n",
    "weather_df_filled = (weather_df\n",
    "                     .withColumn('temp_fwd_filled', ffilled_column)\n",
    "                     .withColumn('temp_bckwd_filled', bfilled_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|               time|location|              temp|   temp_fwd_filled| temp_bckwd_filled|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|2017-09-09 12:00:00|   shade|18.830184076113213|18.830184076113213|18.830184076113213|\n",
      "|2017-09-09 12:01:00|     sun|              null|18.830184076113213| 21.55237663805009|\n",
      "|2017-09-09 12:30:00|   shade|              null|18.830184076113213| 21.55237663805009|\n",
      "|2017-09-09 12:31:00|     sun| 21.55237663805009| 21.55237663805009| 21.55237663805009|\n",
      "|2017-09-09 13:00:00|   shade| 18.59059750682235| 18.59059750682235| 18.59059750682235|\n",
      "|2017-09-09 13:01:00|     sun|              null| 18.59059750682235|22.587784977960474|\n",
      "|2017-09-09 13:30:00|   shade|              null| 18.59059750682235|22.587784977960474|\n",
      "|2017-09-09 13:31:00|     sun|22.587784977960474|22.587784977960474|22.587784977960474|\n",
      "|2017-09-09 14:00:00|   shade|19.101003724324197|19.101003724324197|19.101003724324197|\n",
      "|2017-09-09 14:01:00|     sun|20.548896316341516|20.548896316341516|20.548896316341516|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "weather_df_filled.orderBy('time').show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using LAST_VALUE(), FIRST_VALUE() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|               time|location|              temp|   temp_fwd_filled|  temp_bckd_filled|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|2017-09-09 12:00:00|   shade|18.830184076113213|18.830184076113213|18.830184076113213|\n",
      "|2017-09-09 12:01:00|     sun|              null|18.830184076113213| 21.55237663805009|\n",
      "|2017-09-09 12:30:00|   shade|              null|18.830184076113213| 21.55237663805009|\n",
      "|2017-09-09 12:31:00|     sun| 21.55237663805009| 21.55237663805009| 21.55237663805009|\n",
      "|2017-09-09 13:00:00|   shade| 18.59059750682235| 18.59059750682235| 18.59059750682235|\n",
      "|2017-09-09 13:01:00|     sun|              null| 18.59059750682235|22.587784977960474|\n",
      "|2017-09-09 13:30:00|   shade|              null| 18.59059750682235|22.587784977960474|\n",
      "|2017-09-09 13:31:00|     sun|22.587784977960474|22.587784977960474|22.587784977960474|\n",
      "|2017-09-09 14:00:00|   shade|19.101003724324197|19.101003724324197|19.101003724324197|\n",
      "|2017-09-09 14:01:00|     sun|20.548896316341516|20.548896316341516|20.548896316341516|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT time, location, temp, \n",
    "          |    LAST_VALUE(temp, TRUE) OVER (ORDER BY time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS temp_fwd_filled,\n",
    "          |    FIRST_VALUE(temp, TRUE) OVER (ORDER BY time ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS temp_bckd_filled\n",
    "          |FROM weather\n",
    "          |ORDER BY time\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Forward-fill and Backward-fill missing data with a threshold:\n",
    "\n",
    "Pandas Dataframe also provides the flexibility to provide a threshold for forward and backward fill. Supppose, we do not want to forward fill more than two consecutive rows with null values. In that case we can mention the `threshold` paramater as follows:\n",
    "`\n",
    "df_filled = df.groupby('location').apply(lambda group: group.fillna(method='ffill', threshold=2))\n",
    "`\n",
    "\n",
    "We can very well achieve this using using an integer value with the `preceding` attribute of `window`. For forward fill in Spark we define a window, which is ordered in time, and which includes all the rows bounded by a upper bound up until the current row. We achieve this here simply by selecting the rows in the window as being the `rowsBetween -2` (at most the last two rows up), and `Window.currentRow` (the current row). Similarly for backward fill we define a window, which is ordered in time, and which includes all the rows from the current row to up until to a point bounded by a lower bound. We achieve this here simply by selecting the rows in the window as being the `rowsBetween Window.currentRow` (the current row) and `2 Following` (at most the first two rows down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = [    \n",
    "    ('2017-09-09 12:00:00', 'shade', 18.830184076113213),\n",
    "    ('2017-09-09 12:01:00', 'sun',   None),\n",
    "    ('2017-09-09 12:30:00', 'shade', None),\n",
    "    ('2017-09-09 12:31:00', 'sun',   21.55237663805009),\n",
    "    ('2017-09-09 13:00:00', 'shade', 18.59059750682235),\n",
    "    ('2017-09-09 13:01:00', 'sun',   None),\n",
    "    ('2017-09-09 13:30:00', 'shade', None),\n",
    "    ('2017-09-09 13:31:00', 'sun',   22.587784977960474),\n",
    "    ('2017-09-09 14:00:00', 'shade', None),\n",
    "    ('2017-09-09 14:01:00', 'sun',   20.548896316341516),\n",
    "    ('2017-09-09 15:00:00', 'shade', None),\n",
    "    ('2017-09-09 15:01:00', 'sun',   None),\n",
    "    ('2017-09-09 16:30:00', 'shade', None),\n",
    "    ('2017-09-09 16:31:00', 'sun',   None),\n",
    "    ('2017-09-09 17:00:00', 'shade', 18.101003724324197),\n",
    "    ('2017-09-09 17:01:00', 'sun',   None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(weather, ['time', 'location', 'temp']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------------+\n",
      "|               time|location|              temp|\n",
      "+-------------------+--------+------------------+\n",
      "|2017-09-09 12:00:00|   shade|18.830184076113213|\n",
      "|2017-09-09 12:01:00|     sun|              null|\n",
      "|2017-09-09 12:30:00|   shade|              null|\n",
      "|2017-09-09 12:31:00|     sun| 21.55237663805009|\n",
      "|2017-09-09 13:00:00|   shade| 18.59059750682235|\n",
      "|2017-09-09 13:01:00|     sun|              null|\n",
      "|2017-09-09 13:30:00|   shade|              null|\n",
      "|2017-09-09 13:31:00|     sun|22.587784977960474|\n",
      "|2017-09-09 14:00:00|   shade|              null|\n",
      "|2017-09-09 14:01:00|     sun|20.548896316341516|\n",
      "|2017-09-09 15:00:00|   shade|              null|\n",
      "|2017-09-09 15:01:00|     sun|              null|\n",
      "|2017-09-09 16:30:00|   shade|              null|\n",
      "|2017-09-09 16:31:00|     sun|              null|\n",
      "|2017-09-09 17:00:00|   shade|18.101003724324197|\n",
      "|2017-09-09 17:01:00|     sun|              null|\n",
      "+-------------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using last(), first() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the window\n",
    "fwd_window = (Window.partitionBy('location')\n",
    "               .orderBy('time')\n",
    "               .rowsBetween(-2, Window.currentRow))\n",
    "\n",
    "bckwd_window = (Window.partitionBy('location')\n",
    "               .orderBy('time')\n",
    "               .rowsBetween(Window.currentRow, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the forward-filled column\n",
    "ffilled_column = F.last(weather_df['temp'], ignorenulls=True).over(fwd_window) # True: fill with last non-null\n",
    "bfilled_column = F.first(weather_df['temp'], ignorenulls=True).over(bckwd_window) # True: fill with first non-null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the fill\n",
    "weather_df_filled = weather_df.withColumn('temp_fwd_filled', ffilled_column).withColumn('temp_bckwd_filled', bfilled_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|               time|location|              temp|   temp_fwd_filled| temp_bckwd_filled|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|2017-09-09 12:00:00|   shade|18.830184076113213|18.830184076113213|18.830184076113213|\n",
      "|2017-09-09 12:30:00|   shade|              null|18.830184076113213| 18.59059750682235|\n",
      "|2017-09-09 13:00:00|   shade| 18.59059750682235| 18.59059750682235| 18.59059750682235|\n",
      "|2017-09-09 13:30:00|   shade|              null| 18.59059750682235|              null|\n",
      "|2017-09-09 14:00:00|   shade|              null| 18.59059750682235|              null|\n",
      "|2017-09-09 15:00:00|   shade|              null|              null|18.101003724324197|\n",
      "|2017-09-09 16:30:00|   shade|              null|              null|18.101003724324197|\n",
      "|2017-09-09 17:00:00|   shade|18.101003724324197|18.101003724324197|18.101003724324197|\n",
      "|2017-09-09 12:01:00|     sun|              null|              null| 21.55237663805009|\n",
      "|2017-09-09 12:31:00|     sun| 21.55237663805009| 21.55237663805009| 21.55237663805009|\n",
      "|2017-09-09 13:01:00|     sun|              null| 21.55237663805009|22.587784977960474|\n",
      "|2017-09-09 13:31:00|     sun|22.587784977960474|22.587784977960474|22.587784977960474|\n",
      "|2017-09-09 14:01:00|     sun|20.548896316341516|20.548896316341516|20.548896316341516|\n",
      "|2017-09-09 15:01:00|     sun|              null|20.548896316341516|              null|\n",
      "|2017-09-09 16:31:00|     sun|              null|20.548896316341516|              null|\n",
      "|2017-09-09 17:01:00|     sun|              null|              null|              null|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "weather_df_filled.orderBy('location', 'time').show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Observe how are able to restrict the Forward Filling only to next two rows. If there are three or more consecutive empty rows they remain unfilled. The same logic applies for backward filling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using LAST_VALUE(), FIRST_VALUE() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|               time|location|              temp|   temp_fwd_filled|  temp_bckd_filled|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "|2017-09-09 12:00:00|   shade|18.830184076113213|18.830184076113213|18.830184076113213|\n",
      "|2017-09-09 12:30:00|   shade|              null|18.830184076113213| 18.59059750682235|\n",
      "|2017-09-09 13:00:00|   shade| 18.59059750682235| 18.59059750682235| 18.59059750682235|\n",
      "|2017-09-09 13:30:00|   shade|              null| 18.59059750682235|19.101003724324197|\n",
      "|2017-09-09 14:00:00|   shade|19.101003724324197|19.101003724324197|19.101003724324197|\n",
      "|2017-09-09 12:01:00|     sun|              null|              null| 21.55237663805009|\n",
      "|2017-09-09 12:31:00|     sun| 21.55237663805009| 21.55237663805009| 21.55237663805009|\n",
      "|2017-09-09 13:01:00|     sun|              null| 21.55237663805009|22.587784977960474|\n",
      "|2017-09-09 13:31:00|     sun|22.587784977960474|22.587784977960474|22.587784977960474|\n",
      "|2017-09-09 14:01:00|     sun|20.548896316341516|20.548896316341516|20.548896316341516|\n",
      "+-------------------+--------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT time, location, temp, \n",
    "          |    LAST_VALUE(temp, TRUE) OVER (PARTITION BY location ORDER BY time ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS temp_fwd_filled,\n",
    "          |    FIRST_VALUE(temp, TRUE) OVER (PARTITION BY location ORDER BY time ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING) AS temp_bckd_filled\n",
    "          |FROM weather\n",
    "          |ORDER BY location, time\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "353px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
