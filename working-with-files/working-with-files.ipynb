{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Files with Apache Spark (PySpark)\n",
    "\n",
    "In this notebook we will go through Read and Write various kinds of files such as Comma Separated, Tab Separated, JSON, Custom Key Value Format, Parquet.\n",
    "\n",
    "`\n",
    "@author: Anindya Saha  \n",
    "@email: mail.anindya@gmail.com\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "A brief synopsis of what each UDF use case is and what functionality does it touch on.\n",
    "\n",
    "| Section                                                                             |        Demonstrates |\n",
    "|:------------------------------------------------------------------------------------|:--------------------|\n",
    "|[1. Creating the Spark Session](#1.-Creating-the-Spark-Session)||\n",
    "|[2. Read Key Value format CSV Files](#2.-Read-Key-Value-format-CSV-Files)|Read Custom Key=Value OR Key:Value formatted files|\n",
    "|[3. Read CSV Files](#3.-Read-CSV-Files)|Read Comma Separated|\n",
    "|[4. Read TSV Files](#4.-Read-TSV-Files)|Read Tab Separated|\n",
    "|[5. Read JSON Files](#5.-Read-JSON-Files)|Read JSON Files|\n",
    "|[6. Read Parquet Files](#6.-Read-Parquet-Files)|Read Parquet Files|\n",
    "|[7. Write Files](#7.-Write-Files)||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting random seed for notebook reproducability\n",
    "rnd_seed=42\n",
    "np.random.seed=rnd_seed\n",
    "np.random.set_state=rnd_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following must be set in your .bashrc file\n",
    "#SPARK_HOME=\"/home/ubuntu/spark-2.4.0-bin-hadoop2.7\"\n",
    "#ANACONDA_HOME=\"/home/ubuntu/anaconda3/envs/pyspark\"\n",
    "#PYSPARK_PYTHON=\"$ANACONDA_HOME/bin/python\"\n",
    "#PYSPARK_DRIVER_PYTHON=\"$ANACONDA_HOME/bin/python\"\n",
    "#PYTHONPATH=\"$ANACONDA_HOME/bin/python\"\n",
    "#export PATH=\"$ANACONDA_HOME/bin:$SPARK_HOME/bin:$PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')\n",
    "         .appName('working-with-files')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MISHER:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>working-with-files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dc512e9fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MISHER:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>working-with-files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=working-with-files>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x1dc512f8ef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Key Value format CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. CSV with Key=Value Pairs\n",
    "\n",
    "Reading a CSV file into PySpark that contains the Key=Value pairing, such that Key becomes the column and value is the data of it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample the file to understand the structure:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age=Middle-aged,sex=Male,education=Bachelors,native-country=United-States,race=White,marital-status=Never-married,workclass=State-gov,occupation=Adm-clerical,hours-per-week=Full-time,income=Small,capital-gain=Low,capital-loss=None',\n",
       " 'age=Senior,sex=Male,education=Bachelors,native-country=United-States,race=White,marital-status=Married-civ-spouse,workclass=Self-emp-not-inc,occupation=Exec-managerial,hours-per-week=Part-time,income=Small,capital-gain=None,capital-loss=None']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile('data/census.csv').sample(False, 0.5, rnd_seed).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Develop the parsing logic:**\n",
    "\n",
    "Take a sampl row and develop the tokenization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line='age=Middle-aged,sex=Male,education=Bachelors,native-country=United-States,race=White,marital-status=Never-married,workclass=State-gov,occupation=Adm-clerical,hours-per-week=Full-time,income=Small,capital-gain=Low,capital-loss=None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Middle-aged',\n",
       " 'Male',\n",
       " 'Bachelors',\n",
       " 'United-States',\n",
       " 'White',\n",
       " 'Never-married',\n",
       " 'State-gov',\n",
       " 'Adm-clerical',\n",
       " 'Full-time',\n",
       " 'Small',\n",
       " 'Low',\n",
       " 'None']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[value for token in line.split(',') for key, value in [token.split('=')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Parse Line Function. Both these functions are equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_census(line):\n",
    "    try:\n",
    "        return [token.split('=')[1] for token in line.split(',')]\n",
    "    except:\n",
    "        return [] # handling malformed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_census(line):\n",
    "    try:\n",
    "        return [value for token in line.split(',') for key, value in [token.split('=')]]\n",
    "    except:\n",
    "        return [] # handling malformed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "census_rdd = spark.sparkContext.textFile('data/census.csv').map(parse_census).filter(lambda record: len(record) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Middle-aged',\n",
       "  'Male',\n",
       "  'Bachelors',\n",
       "  'United-States',\n",
       "  'White',\n",
       "  'Never-married',\n",
       "  'State-gov',\n",
       "  'Adm-clerical',\n",
       "  'Full-time',\n",
       "  'Small',\n",
       "  'Low',\n",
       "  'None'],\n",
       " ['Senior',\n",
       "  'Male',\n",
       "  'Bachelors',\n",
       "  'United-States',\n",
       "  'White',\n",
       "  'Married-civ-spouse',\n",
       "  'Self-emp-not-inc',\n",
       "  'Exec-managerial',\n",
       "  'Part-time',\n",
       "  'Small',\n",
       "  'None',\n",
       "  'None']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: string, sex: string, education: string, native-country: string, race: string, marital-status: string, workclass: string, occupation: string, hours-per-week: string, income: string, capital-gain: string, capital-loss: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_df = (spark.createDataFrame(census_rdd)\n",
    "             .toDF('age', 'sex', 'education', 'native-country', 'race', 'marital-status', \n",
    "                   'workclass', 'occupation', 'hours-per-week', 'income', 'capital-gain', 'capital-loss'))\n",
    "census_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- hours-per-week: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- capital-gain: string (nullable = true)\n",
      " |-- capital-loss: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "census_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>native-country</th>\n",
       "      <th>race</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>workclass</th>\n",
       "      <th>occupation</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Middle-aged</td>\n",
       "      <td>Male</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>United-States</td>\n",
       "      <td>White</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Small</td>\n",
       "      <td>Low</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior</td>\n",
       "      <td>Male</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>United-States</td>\n",
       "      <td>White</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>Small</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle-aged</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>United-States</td>\n",
       "      <td>White</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Private</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Small</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior</td>\n",
       "      <td>Male</td>\n",
       "      <td>11th</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Black</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Private</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Small</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Middle-aged</td>\n",
       "      <td>Female</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>Black</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Private</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Small</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           age     sex  education native-country   race      marital-status  \\\n",
       "0  Middle-aged    Male  Bachelors  United-States  White       Never-married   \n",
       "1       Senior    Male  Bachelors  United-States  White  Married-civ-spouse   \n",
       "2  Middle-aged    Male    HS-grad  United-States  White            Divorced   \n",
       "3       Senior    Male       11th  United-States  Black  Married-civ-spouse   \n",
       "4  Middle-aged  Female  Bachelors           Cuba  Black  Married-civ-spouse   \n",
       "\n",
       "          workclass         occupation hours-per-week income capital-gain  \\\n",
       "0         State-gov       Adm-clerical      Full-time  Small          Low   \n",
       "1  Self-emp-not-inc    Exec-managerial      Part-time  Small         None   \n",
       "2           Private  Handlers-cleaners      Full-time  Small         None   \n",
       "3           Private  Handlers-cleaners      Full-time  Small         None   \n",
       "4           Private     Prof-specialty      Full-time  Small         None   \n",
       "\n",
       "  capital-loss  \n",
       "0         None  \n",
       "1         None  \n",
       "2         None  \n",
       "3         None  \n",
       "4         None  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CSV with Key:Value Pairs With Uneven Column Colunt\n",
    "\n",
    "Reading a CSV file into PySpark that contains the Key:Value pairing, such that Key becomes the column and value is the data of it; And not all Rows contain all the Keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name:Pradnya,IP:100.0.0.4,college:SDM,year:2018',\n",
       " 'name:Ram,IP:100.10.10.5,college:BVB,semester:IV,year:2018']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile('data/students.csv').sample(False, 0.5, rnd_seed).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Parse Line Function:**\n",
    "\n",
    "If we know all field names and keys/values do not contain embedded delimiters. then we can probably convert the key/value lines into Row object through RDD's map function.\n",
    "\n",
    "https://stackoverflow.com/questions/50579452/reading-a-csv-file-into-pyspark-that-contains-the-keyvalue-pairing-such-that-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# define a list of all field names\n",
    "columns = ['name', 'IP', 'college', 'semester', 'year']\n",
    "\n",
    "# set Row object\n",
    "def parse_row(line):\n",
    "    # convert line into key/value tuples. strip spaces and lowercase the `k`\n",
    "    z = dict((key.strip().lower(), value.strip().lower()) for token in line.split(',') for key, value in [ token.split(':') ])\n",
    "    # make sure all columns shown in the Row object\n",
    "    return Row(**dict((col.lower(), z[col.lower()] if col.lower() in z else None) for col in columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "students_rdd = spark.sparkContext.textFile('data/students.csv').map(parse_row).filter(lambda record: len(record) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(college='sdm', ip='100.0.0.4', name='pradnya', semester=None, year='2018'),\n",
       " Row(college='bvb', ip='100.10.10.5', name='ram', semester='iv', year='2018')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, ip: string, college: string, semester: string, year: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_df = spark.createDataFrame(students_rdd).toDF('name', 'ip', 'college', 'semester', 'year')\n",
    "students_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ip</th>\n",
       "      <th>college</th>\n",
       "      <th>semester</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sdm</td>\n",
       "      <td>100.0.0.4</td>\n",
       "      <td>pradnya</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bvb</td>\n",
       "      <td>100.10.10.5</td>\n",
       "      <td>ram</td>\n",
       "      <td>iv</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name           ip  college semester  year\n",
       "0  sdm    100.0.0.4  pradnya     None  2018\n",
       "1  bvb  100.10.10.5      ram       iv  2018"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample the file to understand the structure:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " '2010-01-04,213.429998,214.499996,212.38000099999996,214.009998,123432400,27.727039',\n",
       " '2010-01-06,214.379993,215.23,210.750004,210.969995,138040000,27.333178000000004',\n",
       " '2010-01-07,211.75,212.000006,209.050005,210.58,119282800,27.28265',\n",
       " '2010-01-12,209.18999499999998,209.76999500000002,206.419998,207.720001,148614900,26.91211']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile('data/appl_stock.csv').sample(False, 0.5, rnd_seed).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Infer the Schema:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let Spark know about the header and infer the Schema types!\n",
    "appl_stock = spark.read.csv('data/appl_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide the Schema:**\n",
    "\n",
    "We see that the `Date` column does not have any time component so we can use `DateType()` instead of `TimestampType()`. Also the `Open`, `High`, `Low`, `Close` can be of `FloatType()` and `DoubleType()` is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "appl_stock_schema = StructType(\n",
    "                        [StructField('Date', DateType(), False),\n",
    "                         StructField('Open', FloatType(), False),\n",
    "                         StructField('High', FloatType(), False),\n",
    "                         StructField('Low', FloatType(), False),\n",
    "                         StructField('Close', FloatType(), False),\n",
    "                         StructField('Volume', IntegerType(), False),\n",
    "                         StructField('AdjClose', FloatType(), False)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We provide the Schema to Spark\n",
    "appl_stock = spark.read.csv('data/appl_stock.csv', schema=appl_stock_schema, header=True, dateFormat='yyyy-MM-dd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- AdjClose: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+------+---------+---------+---------+\n",
      "|      Date|     Open|  High|   Low|    Close|   Volume| AdjClose|\n",
      "+----------+---------+------+------+---------+---------+---------+\n",
      "|2010-01-04|   213.43| 214.5|212.38|   214.01|123432400| 27.72704|\n",
      "|2010-01-05|214.59999|215.59|213.25|214.37999|150476200|27.774977|\n",
      "|2010-01-06|214.37999|215.23|210.75|   210.97|138040000|27.333178|\n",
      "|2010-01-07|   211.75| 212.0|209.05|   210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.29999| 212.0|209.06|211.98001|111902700|27.464033|\n",
      "+----------+---------+------+------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read TSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample the file to understand the structure:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company\\tPerson\\tSales',\n",
       " 'GOOG\\tSam\\t200.0',\n",
       " 'GOOG\\tFrank\\t340.0',\n",
       " 'MSFT\\tTina\\t600.0',\n",
       " 'APPL\\tChris\\t350.0']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile('data/sales_info.tsv').sample(False, 0.5, rnd_seed).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Infer the Schema:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let Spark know about the header and infer the Schema types!\n",
    "sales_info = spark.read.csv('data/sales_info.tsv', sep='\\t', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_info.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_info.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infered schema looks good. We do not need to provide any schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Read JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample the file to understand the structure:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"name\":\"Michael\"}', '{\"name\":\"Andy\", \"age\":30}']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile('data/people.json').sample(False, 0.5, rnd_seed).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Infer the Schema:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let Spark infer the Schema\n",
    "people = spark.read.json('data/people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide the Schema:**\n",
    "\n",
    "We see that the `Age` column does not need to be long so we can use `IntegerType()` instead of `long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people_schema = StructType(\n",
    "                        [StructField('age', IntegerType(), False),\n",
    "                         StructField('name', StringType(), False)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We provide the Schema to Spark\n",
    "people = spark.read.json('data/people.json', schema=people_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Read Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Apache Parquet` is a columnar storage format and hence cannot be read as textfile. It will throw gibberish character out if we attempt ot do that as shown below. Also with parquet format with do not need to tell spark to inferSchema because the schema already gets written to the parquet file along with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAR1\\x15\\x00\\x15�n\\x15�n,\\x15�\\x1b\\x15\\x00\\x15\\x06\\x15\\x08\\x1c\\x18\\x04',\n",
       " \"C\\x00\\x00\\x18\\x04\\x159\\x00\\x00\\x16\\x00\\x00\\x00\\x00�7�\\x1b\\x03\\x00\\x00\\x00�\\x1b\\x01\\x159\\x00\\x00\\x169\\x00\\x00\\x179\\x00\\x00\\x189\\x00\\x00\\x199\\x00\\x00\\x1c9\\x00\\x00\\x1d9\\x00\\x00\\x1e9\\x00\\x00\\x1f9\\x00\\x00 9\\x00\\x00$9\\x00\\x00%9\\x00\\x00&9\\x00\\x00'9\\x00\\x00*9\\x00\\x00+9\\x00\\x00,9\\x00\\x00-9\\x00\\x00.9\\x00\\x0019\\x00\\x0029\\x00\\x0039\\x00\\x0049\\x00\\x0059\\x00\\x0089\\x00\\x0099\\x00\\x00:9\\x00\\x00;9\\x00\\x00<9\\x00\\x00@9\\x00\\x00A9\\x00\\x00B9\\x00\\x00C9\\x00\\x00F9\\x00\\x00G9\\x00\\x00H9\\x00\\x00I9\\x00\\x00J9\\x00\\x00M9\\x00\\x00N9\\x00\\x00O9\\x00\\x00P9\\x00\\x00Q9\\x00\\x00T9\\x00\\x00U9\\x00\\x00V9\\x00\\x00W9\\x00\\x00X9\\x00\\x00[9\\x00\\x00\\\\9\\x00\\x00]9\\x00\\x00^9\\x00\\x00_9\\x00\\x00b9\\x00\\x00c9\\x00\\x00d9\\x00\\x00e9\\x00\\x00f9\\x00\\x00i9\\x00\\x00j9\\x00\\x00k9\\x00\\x00l9\\x00\\x00p9\\x00\\x00q9\\x00\\x00r9\\x00\\x00s9\\x00\\x00t9\\x00\\x00w9\\x00\\x00x9\\x00\\x00y9\\x00\\x00z9\\x00\\x00{9\\x00\\x00~9\\x00\\x00\\x7f9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00�9\\x00\\x00\\x00:\\x00\\x00\\x03:\\x00\\x00\\x04:\\x00\\x00\\x05:\\x00\\x00\\x06:\\x00\\x00\\x07:\\x00\\x00\\x0b:\\x00\\x00\\x0c:\\x00\\x00\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parquet file cannot be read as testfile\n",
    "spark.sparkContext.textFile('data/appl_stock.parquet').sample(False, 0.5, rnd_seed).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the Parquet File:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apple_stock_parquet = spark.read.parquet('data/appl_stock.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- AdjClose: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple_stock_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Observe we provided the custom schema for apple stocks to be of type Date instead of Timestamp and that is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+------+---------+---------+---------+\n",
      "|      Date|     Open|  High|   Low|    Close|   Volume| AdjClose|\n",
      "+----------+---------+------+------+---------+---------+---------+\n",
      "|2010-01-04|   213.43| 214.5|212.38|   214.01|123432400| 27.72704|\n",
      "|2010-01-05|214.59999|215.59|213.25|214.37999|150476200|27.774977|\n",
      "|2010-01-06|214.37999|215.23|210.75|   210.97|138040000|27.333178|\n",
      "|2010-01-07|   211.75| 212.0|209.05|   210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.29999| 212.0|209.06|211.98001|111902700|27.464033|\n",
      "+----------+---------+------+------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple_stock_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Tab Separated File with Header\n",
    "appl_stock.write.csv('data/appl_stock_csv', sep='\\t', header=True, mode='overwrite')\n",
    "#appl_stock.write.csv('data/appl_stock_csv', sep='\\t', header=False, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Comma Separated File With Header\n",
    "appl_stock.write.csv('data/appl_stock_tsv', header=True, mode='overwrite')\n",
    "#appl_stock.write.csv('data/appl_stock_tsv', header=False, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write JSON File\n",
    "appl_stock.write.csv('data/appl_stock_json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Parquet File\n",
    "appl_stock.write.parquet('data/appl_stock_parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
